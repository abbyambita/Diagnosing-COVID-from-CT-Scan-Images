{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "trial_saganv2_git.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abbyambita/Diagnosing-COVID-from-CT-Scan-Images/blob/main/trial_saganv2_git.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axVHKs9vwqad"
      },
      "source": [
        "https://github.com/rosinality/sagan-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrbY_X-nvYFM",
        "outputId": "805a2884-0eac-4325-face-0443e3643c3a"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncGTysumvhSl",
        "outputId": "e0bb41a8-b2f2-4e10-cf9f-d9b71c606824"
      },
      "source": [
        "import os \r\n",
        "\r\n",
        "os.chdir(\"/content/gdrive/My Drive\")\r\n",
        "\r\n",
        "!ls  '/content/gdrive/My Drive/CS 284 Mini-Project/Code/output_result/sagan/git/batch_size=64,epoch=3000'\r\n",
        "\r\n",
        "%cd \"/content/gdrive/My Drive/CS 284 Mini-Project/Code/\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0000100.png  discriminator_epoch_199.pth  generator_epoch_199.pth\n",
            "0000200.png  discriminator_epoch_299.pth  generator_epoch_299.pth\n",
            "0000300.png  discriminator_epoch_399.pth  generator_epoch_399.pth\n",
            "0000400.png  discriminator_epoch_499.pth  generator_epoch_499.pth\n",
            "0000500.png  discriminator_epoch_99.pth   generator_epoch_99.pth\n",
            "/content/gdrive/.shortcut-targets-by-id/1eVFVz23F6ROX0s10Oe3tT9HVzr502iW2/CS 284 Mini-Project/Code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu35A4u7wEDx"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "#%matplotlib inline\r\n",
        "import argparse\r\n",
        "import os\r\n",
        "import PIL\r\n",
        "import glob\r\n",
        "import xml.etree.ElementTree as ET\r\n",
        "import random\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.parallel\r\n",
        "import torch.backends.cudnn as cudnn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data\r\n",
        "import torchvision.datasets as dset\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.utils as vutils\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.animation as animation\r\n",
        "import seaborn as sns\r\n",
        "from IPython.display import HTML\r\n",
        "from torchvision.utils import save_image\r\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\r\n",
        "from tqdm import tqdm_notebook as tqdm\r\n",
        "from IPython.display import clear_output\r\n",
        "from scipy.stats import truncnorm\r\n",
        "%matplotlib inline\r\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\r\n",
        "\r\n",
        "import torch\r\n",
        "\r\n",
        "from torch import nn\r\n",
        "from torch.nn import init\r\n",
        "from torch.nn import functional as F\r\n",
        "\r\n",
        "import functools\r\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF2S8MPFYpIA"
      },
      "source": [
        "def init_linear(linear):\r\n",
        "    init.xavier_uniform_(linear.weight)\r\n",
        "    linear.bias.data.zero_()\r\n",
        "\r\n",
        "\r\n",
        "def init_conv(conv, glu=True):\r\n",
        "    init.xavier_uniform_(conv.weight)\r\n",
        "    if conv.bias is not None:\r\n",
        "        conv.bias.data.zero_()\r\n",
        "\r\n",
        "\r\n",
        "class SpectralNorm:\r\n",
        "    def __init__(self, name):\r\n",
        "        self.name = name\r\n",
        "\r\n",
        "    def compute_weight(self, module):\r\n",
        "        weight = getattr(module, self.name + '_orig')\r\n",
        "        u = getattr(module, self.name + '_u')\r\n",
        "        size = weight.size()\r\n",
        "        weight_mat = weight.contiguous().view(size[0], -1)\r\n",
        "        with torch.no_grad():\r\n",
        "            v = weight_mat.t() @ u\r\n",
        "            v = v / v.norm()\r\n",
        "            u = weight_mat @ v\r\n",
        "            u = u / u.norm()\r\n",
        "        sigma = u @ weight_mat @ v\r\n",
        "        weight_sn = weight / sigma\r\n",
        "        # weight_sn = weight_sn.view(*size)\r\n",
        "\r\n",
        "        return weight_sn, u\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def apply(module, name):\r\n",
        "        fn = SpectralNorm(name)\r\n",
        "\r\n",
        "        weight = getattr(module, name)\r\n",
        "        del module._parameters[name]\r\n",
        "        module.register_parameter(name + '_orig', weight)\r\n",
        "        input_size = weight.size(0)\r\n",
        "        u = weight.new_empty(input_size).normal_()\r\n",
        "        module.register_buffer(name, weight)\r\n",
        "        module.register_buffer(name + '_u', u)\r\n",
        "\r\n",
        "        module.register_forward_pre_hook(fn)\r\n",
        "\r\n",
        "        return fn\r\n",
        "\r\n",
        "    def __call__(self, module, input):\r\n",
        "        weight_sn, u = self.compute_weight(module)\r\n",
        "        setattr(module, self.name, weight_sn)\r\n",
        "        setattr(module, self.name + '_u', u)\r\n",
        "\r\n",
        "\r\n",
        "def spectral_norm(module, name='weight'):\r\n",
        "    SpectralNorm.apply(module, name)\r\n",
        "\r\n",
        "    return module\r\n",
        "\r\n",
        "\r\n",
        "def spectral_init(module, gain=1):\r\n",
        "    init.kaiming_uniform_(module.weight, gain)\r\n",
        "    if module.bias is not None:\r\n",
        "        module.bias.data.zero_()\r\n",
        "\r\n",
        "    return spectral_norm(module)\r\n",
        "\r\n",
        "\r\n",
        "def leaky_relu(input):\r\n",
        "    return F.leaky_relu(input, negative_slope=0.2)\r\n",
        "\r\n",
        "\r\n",
        "class SelfAttention(nn.Module):\r\n",
        "    def __init__(self, in_channel, gain=1):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.query = spectral_init(nn.Conv1d(in_channel, in_channel // 8, 1),\r\n",
        "                                   gain=gain)\r\n",
        "        self.key = spectral_init(nn.Conv1d(in_channel, in_channel // 8, 1),\r\n",
        "                                 gain=gain)\r\n",
        "        self.value = spectral_init(nn.Conv1d(in_channel, in_channel, 1),\r\n",
        "                                   gain=gain)\r\n",
        "\r\n",
        "        self.gamma = nn.Parameter(torch.tensor(0.0))\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        shape = input.shape\r\n",
        "        flatten = input.view(shape[0], shape[1], -1)\r\n",
        "        query = self.query(flatten).permute(0, 2, 1)\r\n",
        "        key = self.key(flatten)\r\n",
        "        value = self.value(flatten)\r\n",
        "        query_key = torch.bmm(query, key)\r\n",
        "        attn = F.softmax(query_key, 1)\r\n",
        "        attn = torch.bmm(value, attn)\r\n",
        "        attn = attn.view(*shape)\r\n",
        "        out = self.gamma * attn + input\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class ConditionalNorm(nn.Module):\r\n",
        "    def __init__(self, in_channel, n_class):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.bn = nn.BatchNorm2d(in_channel, affine=False)\r\n",
        "        self.embed = nn.Embedding(n_class, in_channel * 2)\r\n",
        "        self.embed.weight.data[:, :in_channel] = 1\r\n",
        "        self.embed.weight.data[:, in_channel:] = 0\r\n",
        "\r\n",
        "    def forward(self, input, class_id):\r\n",
        "        out = self.bn(input)\r\n",
        "        embed = self.embed(class_id)\r\n",
        "        gamma, beta = embed.chunk(2, 1)\r\n",
        "        gamma = gamma.unsqueeze(2).unsqueeze(3)\r\n",
        "        beta = beta.unsqueeze(2).unsqueeze(3)\r\n",
        "        out = gamma * out + beta\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class ConvBlock(nn.Module):\r\n",
        "    def __init__(self, in_channel, out_channel, kernel_size=[3, 3],\r\n",
        "                 padding=1, stride=1, n_class=None, bn=True,\r\n",
        "                 activation=F.relu, upsample=True, self_attention=False):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.conv = spectral_init(nn.Conv2d(in_channel, out_channel,\r\n",
        "                                            kernel_size, stride, padding,\r\n",
        "                                            bias=False if bn else True))\r\n",
        "\r\n",
        "        self.upsample = upsample\r\n",
        "        self.activation = activation\r\n",
        "        self.bn = bn\r\n",
        "        if bn:\r\n",
        "            self.norm = ConditionalNorm(out_channel, n_class)\r\n",
        "\r\n",
        "        self.self_attention = self_attention\r\n",
        "        if self_attention:\r\n",
        "            self.attention = SelfAttention(out_channel, 1)\r\n",
        "\r\n",
        "    def forward(self, input, class_id=None):\r\n",
        "        out = input\r\n",
        "        if self.upsample:\r\n",
        "            out = F.upsample(out, scale_factor=2)\r\n",
        "\r\n",
        "        out = self.conv(out)\r\n",
        "\r\n",
        "        if self.bn:\r\n",
        "            out = self.norm(out, class_id)\r\n",
        "\r\n",
        "        if self.activation is not None:\r\n",
        "            out = self.activation(out)\r\n",
        "\r\n",
        "        if self.self_attention:\r\n",
        "            out = self.attention(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class Generator(nn.Module):\r\n",
        "    def __init__(self, code_dim=100, n_class=2):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.lin_code = spectral_init(nn.Linear(code_dim, 4 * 4 * 512))\r\n",
        "        self.conv = nn.ModuleList([ConvBlock(512, 512, n_class=n_class),\r\n",
        "                                   ConvBlock(512, 512, n_class=n_class),\r\n",
        "                                   ConvBlock(512, 512, n_class=n_class,\r\n",
        "                                             self_attention=True),\r\n",
        "                                   ConvBlock(512, 256, n_class=n_class),\r\n",
        "                                   ConvBlock(256, 128, n_class=n_class)])\r\n",
        "\r\n",
        "        self.colorize = spectral_init(nn.Conv2d(128, 3, [3, 3], padding=1))\r\n",
        "\r\n",
        "    def forward(self, input, class_id):\r\n",
        "        out = self.lin_code(input)\r\n",
        "        out = F.relu(out)\r\n",
        "        out = out.view(-1, 512, 4, 4)\r\n",
        "\r\n",
        "        for conv in self.conv:\r\n",
        "            out = conv(out, class_id)\r\n",
        "\r\n",
        "        out = self.colorize(out)\r\n",
        "\r\n",
        "        return F.tanh(out)\r\n",
        "\r\n",
        "\r\n",
        "class Discriminator(nn.Module):\r\n",
        "    def __init__(self, n_class=2):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        def conv(in_channel, out_channel, stride=2,\r\n",
        "                 self_attention=False):\r\n",
        "            return ConvBlock(in_channel, out_channel, stride=stride,\r\n",
        "                             bn=False, activation=leaky_relu,\r\n",
        "                             upsample=False, self_attention=self_attention)\r\n",
        "\r\n",
        "        self.conv = nn.Sequential(conv(3, 128),\r\n",
        "                                  conv(128, 256),\r\n",
        "                                  conv(256, 512, stride=1,\r\n",
        "                                       self_attention=True),\r\n",
        "                                  conv(512, 512),\r\n",
        "                                  conv(512, 512),\r\n",
        "                                  conv(512, 512))\r\n",
        "\r\n",
        "        self.linear = spectral_init(nn.Linear(512, 1))\r\n",
        "\r\n",
        "        self.embed = nn.Embedding(n_class, 512)\r\n",
        "        self.embed.weight.data.uniform_(-0.1, 0.1)\r\n",
        "        self.embed = spectral_norm(self.embed)\r\n",
        "\r\n",
        "    def forward(self, input, class_id):\r\n",
        "        out = self.conv(input)\r\n",
        "        out = out.view(out.size(0), out.size(1), -1)\r\n",
        "        out = out.sum(2)\r\n",
        "        out_linear = self.linear(out).squeeze(1)\r\n",
        "        embed = self.embed(class_id)\r\n",
        "        prod = (out * embed).sum(1)\r\n",
        "\r\n",
        "        return out_linear + prod"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK7E49C8bGYB"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "import numpy as np\r\n",
        "import glob\r\n",
        "import os\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import argparse\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch import nn, optim\r\n",
        "from torch.nn import functional as F\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import datasets, transforms, utils"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FZ6-P4zh2Ax"
      },
      "source": [
        "batch=64\r\n",
        "steps = 100\r\n",
        "code=128\r\n",
        "lr_g=1e-4\r\n",
        "lr_d=4e-4\r\n",
        "n_d=1\r\n",
        "model='dcgan'\r\n",
        "path='revised-git/train'\r\n",
        "n_class = 2\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "transform = transforms.Compose(\r\n",
        "    [\r\n",
        "        transforms.Resize(128),\r\n",
        "        transforms.CenterCrop(128),\r\n",
        "        transforms.RandomHorizontalFlip(),\r\n",
        "        transforms.ToTensor(),\r\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "def requires_grad(model, flag=True):\r\n",
        "    for p in model.parameters():\r\n",
        "        p.requires_grad = flag\r\n",
        "\r\n",
        "\r\n",
        "def sample_data(path, batch_size):\r\n",
        "    \r\n",
        "    #dataroot = \"revised-git/train\"\r\n",
        "\r\n",
        "    dataset = datasets.ImageFolder(path, transform=transform)\r\n",
        "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size, num_workers=4)\r\n",
        "    \r\n",
        "    return loader\r\n",
        "    #loader = iter(loader)\r\n",
        "\r\n",
        "    # while True:\r\n",
        "    #     try:\r\n",
        "    #         yield next(loader)\r\n",
        "\r\n",
        "    #     except StopIteration:\r\n",
        "    #         loader = DataLoader(\r\n",
        "    #             dataset, shuffle=True, batch_size=batch_size, num_workers=4\r\n",
        "    #         )\r\n",
        "    #         loader = iter(loader)\r\n",
        "    #         yield next(loader)\r\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M2S0mWGrpLp"
      },
      "source": [
        "num_epochs = 3500\r\n",
        "\r\n",
        "dir = \"output_result/sagan/git/batch_size=64,epoch=\"+str(num_epochs)\r\n",
        "os.makedirs(dir, exist_ok=True)\r\n",
        "\r\n",
        "\r\n",
        "def cuda(data):\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        return data.cuda()\r\n",
        "    else:\r\n",
        "        return data\r\n",
        "\r\n",
        "fixed_z = cuda(torch.randn(64, 100))\r\n",
        "\r\n",
        "def denorm(x):\r\n",
        "    out = (x + 1) / 2\r\n",
        "    return out.clamp_(0, 1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTDalXrrkw8u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myy-aGGbiqDc"
      },
      "source": [
        "def train(n_class, generator, discriminator):\r\n",
        "    dataset = sample_data(path,batch)\r\n",
        "    #pbar = tqdm(range(iter), dynamic_ncols=True)\r\n",
        "\r\n",
        "    requires_grad(generator, False)\r\n",
        "    requires_grad(discriminator, True)\r\n",
        "\r\n",
        "    preset_code = torch.randn(n_class * 5, code).to(device)\r\n",
        "\r\n",
        "    disc_loss_val = 0\r\n",
        "    gen_loss_val = 0\r\n",
        "\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "      for i, data in enumerate(dataset):\r\n",
        "        discriminator.zero_grad()\r\n",
        "        #real_image, label = next(dataset)\r\n",
        "        real_image = data[0]\r\n",
        "        label = data[1]\r\n",
        "        real_image = real_image.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        b_size = real_image.size(0)\r\n",
        "        \r\n",
        "\r\n",
        "        fake_image = generator(\r\n",
        "            torch.randn(b_size, code).to(device), label.to(device)\r\n",
        "        )\r\n",
        "        \r\n",
        "        fake_predict = discriminator(fake_image, label)\r\n",
        "        real_predict = discriminator(real_image, label)\r\n",
        "        loss = F.relu(1 + fake_predict).mean()\r\n",
        "\r\n",
        "        loss = loss + F.relu(1 - real_predict).mean()\r\n",
        "        disc_loss_val = loss.detach().item()\r\n",
        "        loss.backward()\r\n",
        "        d_optimizer.step()\r\n",
        "\r\n",
        "        generator.zero_grad()\r\n",
        "        requires_grad(generator, True)\r\n",
        "        requires_grad(discriminator, False)\r\n",
        "        input_class = torch.multinomial(\r\n",
        "            torch.ones(n_class), batch, replacement=True\r\n",
        "        ).to(device)\r\n",
        "        fake_image = generator(\r\n",
        "            torch.randn(batch, code).to(device), input_class\r\n",
        "        )\r\n",
        "        predict = discriminator(fake_image, input_class)\r\n",
        "        loss = -predict.mean()\r\n",
        "        gen_loss_val = loss.detach().item()\r\n",
        "        loss.backward()\r\n",
        "        g_optimizer.step()\r\n",
        "        requires_grad(generator, False)\r\n",
        "        requires_grad(discriminator, True)\r\n",
        "\r\n",
        "        # if (epoch + 1) % 2 == 0:\r\n",
        "        #     generator.train(False)\r\n",
        "        #     input_class = torch.arange(n_class).long().repeat(5).to(device)\r\n",
        "        #     fake_image = generator(preset_code, input_class)\r\n",
        "        #     generator.train(True)\r\n",
        "        #     utils.save_image(\r\n",
        "        #         fake_image.cpu().data,\r\n",
        "        #         #f'sample/{str(i + 1).zfill(7)}.png',\r\n",
        "        #         f'output_result/sagan/batch_size=64,epoch=1000/{str(i + 1).zfill(7)}.png',\r\n",
        "        #         nrow=n_class,\r\n",
        "        #         normalize=True,\r\n",
        "        #         range=(-1, 1),\r\n",
        "        #     )\r\n",
        "\r\n",
        "        print(\"Epoch \"+str(epoch)+\"Dis \"+str(disc_loss_val))\r\n",
        "\r\n",
        "        if (epoch + 1) % (100) == 0:\r\n",
        "            input_class = torch.arange(n_class).long().repeat(5).to(device)\r\n",
        "            fake_image = generator(preset_code, input_class)\r\n",
        "            utils.save_image(\r\n",
        "                fake_image.cpu().data,\r\n",
        "                #f'sample/{str(i + 1).zfill(7)}.png',\r\n",
        "                f'output_result/sagan/git/batch_size=64,epoch=3500/{str(epoch + 1).zfill(7)}.png',\r\n",
        "                nrow=n_class,\r\n",
        "                normalize=True,\r\n",
        "                range=(-1, 1),\r\n",
        "            )\r\n",
        "\r\n",
        "            torch.save(generator, dir+\"/generator_epoch_\"+str(epoch)+\".pth\")\r\n",
        "            torch.save(discriminator, dir+\"/discriminator_epoch_\"+str(epoch)+\".pth\")\r\n",
        "\r\n",
        "        # if (epoch + 1) % (500) == 0:\r\n",
        "        #     generate_images(epoch, 2000)\r\n",
        "        \r\n",
        "\r\n",
        "        # if (i + 1) % 10000 == 0:\r\n",
        "        #     no = str(i + 1).zfill(7)\r\n",
        "        #     torch.save(generator.state_dict(), f'checkpoint/generator_{no}.pt')\r\n",
        "        #     torch.save(discriminator.state_dict(), f'checkpoint/discriminator_{no}.pt')\r\n",
        "        #     torch.save(g_optimizer.state_dict(), f'checkpoint/gen_optimizer_{no}.pt')\r\n",
        "        #     torch.save(d_optimizer.state_dict(), f'checkpoint/dis_optimizer_{no}.pt')\r\n",
        "\r\n",
        "        # pbar.set_description(\r\n",
        "        #     (f'{i + 1}; G: {gen_loss_val:.5f};' f' D: {disc_loss_val:.5f}')\r\n",
        "        # )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u34b-RV-iskV"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "\r\n",
        "    n_class = len(glob.glob(os.path.join(path, '*/')))\r\n",
        "    print(n_class)\r\n",
        "\r\n",
        "    # if model == 'dcgan':\r\n",
        "    #     from model import Generator, Discriminator\r\n",
        "\r\n",
        "    # elif model == 'resnet':\r\n",
        "    #     from model_resnet import Generator, Discriminator\r\n",
        "\r\n",
        "    generator = Generator(code, n_class).to(device)\r\n",
        "    discriminator = Discriminator(n_class).to(device)\r\n",
        "\r\n",
        "    g_optimizer = optim.Adam(generator.parameters(), lr=lr_g, betas=(0, 0.9))\r\n",
        "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0, 0.9))\r\n",
        "    train(n_class, generator, discriminator)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q2j7Q_RzPhj"
      },
      "source": [
        "generator = torch.load(\"output_result/sagan/git/batch_size=64,epoch=3500/generator_epoch_1499.pth\")"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-85RxRo12Kgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f1e8fc-7b46-4573-da56-a55fd41c0492"
      },
      "source": [
        "def generate_images(epoch, batch_size):\r\n",
        "  input_class = torch.arange(n_class).long().repeat(batch_size).to(device)\r\n",
        "  print(len(input_class))\r\n",
        "\r\n",
        "  preset_code = torch.randn(n_class * batch_size, code).to(device)\r\n",
        "  print(len(preset_code))\r\n",
        "  \r\n",
        "  fake_image = generator(preset_code, input_class)\r\n",
        "  print(len(fake_image))\r\n",
        "\r\n",
        "  ncv = \"sagan_output_images/git/batch_size=64,epoch=\"+str(epoch)+\"/noncovid\"\r\n",
        "  cv = \"sagan_output_images/git/batch_size=64,epoch=\"+str(epoch)+\"/covid\"\r\n",
        "\r\n",
        "  os.makedirs(ncv, exist_ok=True)\r\n",
        "  os.makedirs(cv, exist_ok=True)\r\n",
        "\r\n",
        "  for i, img in enumerate(fake_image):\r\n",
        "    if(input_class[i]==0):\r\n",
        "      utils.save_image(\r\n",
        "                img.cpu().data,\r\n",
        "                #f'sample/{str(i + 1).zfill(7)}.png',\r\n",
        "                ncv+'/b15_fake_img_'+str(i)+'.png'\r\n",
        "            )\r\n",
        "    else:\r\n",
        "      utils.save_image(\r\n",
        "                img.cpu().data,\r\n",
        "                #f'sample/{str(i + 1).zfill(7)}.png',\r\n",
        "                cv+'/b15_fake_img_'+str(i)+'.png'\r\n",
        "             )\r\n",
        "\r\n",
        "generate_images(1500, 100)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n",
            "200\n",
            "200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2952: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}