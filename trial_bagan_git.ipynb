{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "trial_bagan_git.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abbyambita/Diagnosing-COVID-from-CT-Scan-Images/blob/main/trial_bagan_git.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrbY_X-nvYFM",
        "outputId": "380ed781-f216-4539-8f54-b2e4f55c7713"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncGTysumvhSl",
        "outputId": "c795b8f3-6a9d-4ffb-eca1-45c9f329eddd"
      },
      "source": [
        "import os \r\n",
        "\r\n",
        "os.chdir(\"/content/gdrive/My Drive\")\r\n",
        "\r\n",
        "#!ls  '/content/gdrive/My Drive/CS 284 Mini-Project/Code/'\r\n",
        "\r\n",
        "%cd \"/content/gdrive/My Drive/CS 284 Mini-Project/Code/\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1eVFVz23F6ROX0s10Oe3tT9HVzr502iW2/CS 284 Mini-Project/Code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu35A4u7wEDx"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "#%matplotlib inline\r\n",
        "import argparse\r\n",
        "import os\r\n",
        "import PIL\r\n",
        "import glob\r\n",
        "import xml.etree.ElementTree as ET\r\n",
        "import random\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.parallel\r\n",
        "import torch.backends.cudnn as cudnn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data\r\n",
        "import torchvision.datasets as dset\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.utils as vutils\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.animation as animation\r\n",
        "import seaborn as sns\r\n",
        "from IPython.display import HTML\r\n",
        "from torchvision.utils import save_image\r\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\r\n",
        "from tqdm import tqdm_notebook as tqdm\r\n",
        "from IPython.display import clear_output\r\n",
        "from scipy.stats import truncnorm\r\n",
        "%matplotlib inline\r\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\r\n",
        "\r\n",
        "from easydict import EasyDict as edict\r\n",
        "from PIL import Image\r\n",
        "from collections import OrderedDict\r\n",
        "import yaml\r\n",
        "import imageio\r\n",
        "import numpy as np\r\n",
        "import torch.utils.data as data\r\n",
        "import torchvision.datasets as datasets\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import matplotlib\r\n",
        "matplotlib.use('Agg')\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import sys\r\n",
        "import time\r\n",
        "import math\r\n",
        "\r\n",
        "import scipy.misc\r\n",
        "import torchvision\r\n",
        "import torchvision.datasets as datasets\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Ylm3_NuanS"
      },
      "source": [
        "workers = 6\r\n",
        "display= 10\r\n",
        "epoches= 20\r\n",
        "test_epoches= 30\r\n",
        "batch_size= 32\r\n",
        "base_lr= 0.0002\r\n",
        "beta1= 0.5\r\n",
        "ano_para= 0.1\r\n",
        "image_size= 64 \r\n",
        "z_dim= 100\r\n",
        "c_dim= 3\r\n",
        "gf_dim= 64\r\n",
        "df_dim= 64\r\n",
        "#class_num= 10\r\n",
        "class_num= 2\r\n",
        "pretrained = True\r\n",
        "\r\n",
        "dir = \"epochs=\"+str(epoches)+\"_lr=\"+str(base_lr)+\"_batch_size=\"+str(batch_size)\r\n",
        "os.makedirs(\"model_result/bagan/git/\"+dir, exist_ok=True)\r\n",
        "os.makedirs(\"plots/bagan/git/\"+dir, exist_ok=True)\r\n",
        "os.makedirs(\"model_backup/bagan/git/\"+dir, exist_ok=True)\r\n",
        "os.makedirs(\"bagan_output_images/git/\"+dir, exist_ok=True)\r\n",
        "os.makedirs(\"model_result/bagan/git/\"+dir+\"/distribution\", exist_ok=True)\r\n",
        "os.makedirs(\"bagan_output_images/git/\"+dir+\"/accuracy\", exist_ok=True)\r\n",
        "os.makedirs(\"bagan_output_images/git/\"+dir+\"/loss\", exist_ok=True)\r\n",
        "os.makedirs(\"model_backup/bagan/git/\"+dir+\"/gan\", exist_ok=True)\r\n",
        "os.makedirs(\"model_backup/bagan/git/\"+dir+\"/vae\", exist_ok=True)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "checkpoint_dir = \"model_backup/bagan/git/\"+dir\r\n",
        "save_dir = \"bagan_output_images/git/\"+dir\r\n",
        "distribution_dir =\"model_result/bagan/git/\"+dir+\"/distribution\""
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-tWCNsN0c-g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3QUSZYhy9T5"
      },
      "source": [
        "def load_images(batch_size, image_size):\r\n",
        "  dataroot = \"revised-git/train\"\r\n",
        "\r\n",
        "  dataset = dset.ImageFolder(root=dataroot,\r\n",
        "                            transform=transforms.Compose([\r\n",
        "                                transforms.Resize(image_size),\r\n",
        "                                transforms.RandomHorizontalFlip(),\r\n",
        "                                transforms.CenterCrop(image_size),\r\n",
        "                                transforms.ToTensor(),\r\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r\n",
        "                            ]))\r\n",
        "  \r\n",
        "  \r\n",
        "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True, num_workers=workers)\r\n",
        "\r\n",
        "  return dataloader"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EJwL7DAzXXa",
        "outputId": "d4c53662-3ab0-4f37-ab85-ec508ec80301"
      },
      "source": [
        "dataloader = load_images(32, 128)\r\n",
        "print(len(dataloader))\r\n",
        "# real_batch = iter(dataloader).next()\r\n",
        "# plt.figure(figsize=(15,15))\r\n",
        "# plt.axis(\"off\")\r\n",
        "# plt.title(\"Training Images\")\r\n",
        "# image = np.transpose(vutils.make_grid(real_batch[0].to(device), normalize=True).cpu(),axes=(1,2,0))\r\n",
        "# plt.imshow(image)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoXkR1rPnm70"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D58t3IfNW_pC"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "# Generator\r\n",
        "class Decoder(nn.Module):\r\n",
        "\tdef __init__(self, z_dim, c_dim, gf_dim):\r\n",
        "\t\tsuper(Decoder, self).__init__()\r\n",
        "\r\n",
        "\t\tself.convTrans0 = nn.ConvTranspose2d(z_dim, gf_dim*8, 4, 1, 0, bias=False)\r\n",
        "\t\tself.bn0 = nn.BatchNorm2d(gf_dim*8)\r\n",
        "\t\tself.relu0 = nn.ReLU(inplace=True)\r\n",
        "\t\t\r\n",
        "\t\tself.convTrans1 = nn.ConvTranspose2d(gf_dim*8, gf_dim*4, 4, 2, 1, bias=False)\r\n",
        "\t\tself.bn1 = nn.BatchNorm2d(gf_dim*4)\r\n",
        "\t\tself.relu1 = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "\t\tself.convTrans2 = nn.ConvTranspose2d(gf_dim*4, gf_dim*2, 4, 2, 1, bias=False)\r\n",
        "\t\tself.bn2 = nn.BatchNorm2d(gf_dim*2)\r\n",
        "\t\tself.relu2 = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "\t\tself.convTrans3 = nn.ConvTranspose2d(gf_dim*2, gf_dim, 4, 2, 1, bias=False)\r\n",
        "\t\tself.bn3 = nn.BatchNorm2d(gf_dim)\r\n",
        "\t\tself.relu3 = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "\t\tself.convTrans4 = nn.ConvTranspose2d(gf_dim, c_dim, 4, 2, 1, bias=False)\r\n",
        "\t\tself.tanh = nn.Tanh()\r\n",
        "\r\n",
        "\t\tfor m in self.modules():\r\n",
        "\t\t\t\tif isinstance(m, nn.ConvTranspose2d):\r\n",
        "\t\t\t\t\t\tm.weight.data.normal_(0.0, 0.02)\r\n",
        "\t\t\t\t\t\tif m.bias is not None:\r\n",
        "\t\t\t\t\t\t\t\tm.bias.data.zero_()\r\n",
        "\r\n",
        "\tdef forward(self, z):\r\n",
        "\t\th0 = self.relu0(self.bn0(self.convTrans0(z)))\r\n",
        "\t\th1 = self.relu1(self.bn1(self.convTrans1(h0)))\r\n",
        "\t\th2 = self.relu2(self.bn2(self.convTrans2(h1)))\r\n",
        "\t\th3 = self.relu3(self.bn3(self.convTrans3(h2)))\r\n",
        "\t\th4 = self.convTrans4(h3)\r\n",
        "\t\toutput = self.tanh(h4)\r\n",
        "\t\treturn output # (c_dim, 64, 64)\r\n",
        "\r\n",
        "# Discriminator\r\n",
        "class Encoder(nn.Module): \r\n",
        "\tdef __init__(self, z_dim, c_dim, df_dim):\r\n",
        "\t\tsuper(Encoder, self).__init__()\r\n",
        "\t\tself.df_dim = df_dim\r\n",
        "\r\n",
        "\t\tself.conv0 = nn.Conv2d(c_dim, df_dim, 4, 2, 1, bias=False)\r\n",
        "\t\tself.relu0 = nn.LeakyReLU(0.2, inplace=True)\r\n",
        "\t\t\r\n",
        "\t\tself.conv1 = nn.Conv2d(df_dim, df_dim*2, 4, 2, 1, bias=False)\r\n",
        "\t\tself.bn1 = nn.BatchNorm2d(df_dim*2)\r\n",
        "\t\tself.relu1 = nn.LeakyReLU(0.2, inplace=True)\r\n",
        "\r\n",
        "\t\tself.conv2 = nn.Conv2d(df_dim*2, df_dim*4, 4, 2, 1, bias=False)\r\n",
        "\t\tself.bn2 = nn.BatchNorm2d(df_dim*4)\r\n",
        "\t\tself.relu2 = nn.LeakyReLU(0.2, inplace=True)\r\n",
        "\r\n",
        "\t\tself.conv3 = nn.Conv2d(df_dim*4, df_dim*8, 4, 2, 1, bias=False)\r\n",
        "\t\tself.bn3 = nn.BatchNorm2d(df_dim*8)\r\n",
        "\t\tself.relu3 = nn.LeakyReLU(0.2, inplace=True)\r\n",
        "\r\n",
        "\t\tself.fc_z1 = nn.Linear(df_dim*8*4*4, z_dim)\r\n",
        "\t\tself.fc_z2 = nn.Linear(df_dim*8*4*4, z_dim)\r\n",
        "\r\n",
        "\t\t#self.conv4 = nn.Conv2d(df_dim*8, 1, 4, 1, 0, bias=False)\r\n",
        "\r\n",
        "\t\tfor m in self.modules():\r\n",
        "\t\t\t\tif isinstance(m, nn.Conv2d):\r\n",
        "\t\t\t\t\t\tm.weight.data.normal_(0.0, 0.02)\r\n",
        "\t\t\t\t\t\tif m.bias is not None:\r\n",
        "\t\t\t\t\t\t\t\tm.bias.data.zero_()\r\n",
        "\r\n",
        "\tdef forward(self, input):\r\n",
        "\t\th0 = self.relu0(self.conv0(input))\r\n",
        "\t\th1 = self.relu1(self.bn1(self.conv1(h0)))\r\n",
        "\t\th2 = self.relu2(self.bn2(self.conv2(h1)))\r\n",
        "\t\th3 = self.relu3(self.bn3(self.conv3(h2)))\r\n",
        "\t\t\r\n",
        "\t\tmu = self.fc_z1(h3.view(-1, self.df_dim*8*4*4))\t# (1, 128*8*4*4)\r\n",
        "\t\tsigma = self.fc_z2(h3.view(-1, self.df_dim*8*4*4))\r\n",
        "\t\treturn mu,sigma # by squeeze, get just float not float Tenosor\r\n",
        "\r\n",
        "\r\n",
        "class Generator(nn.Module):\r\n",
        "        def __init__(self, z_dim, c_dim, gf_dim):\r\n",
        "                super(Generator, self).__init__()\r\n",
        "\r\n",
        "                self.convTrans0 = nn.ConvTranspose2d(z_dim, gf_dim*8, 4, 1, 0, bias=False)\r\n",
        "                self.bn0 = nn.BatchNorm2d(gf_dim*8)\r\n",
        "                self.relu0 = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "                self.convTrans1 = nn.ConvTranspose2d(gf_dim*8, gf_dim*4, 4, 2, 1, bias=False)\r\n",
        "                self.bn1 = nn.BatchNorm2d(gf_dim*4)\r\n",
        "                self.relu1 = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "                self.convTrans2 = nn.ConvTranspose2d(gf_dim*4, gf_dim*2, 4, 2, 1, bias=False)\r\n",
        "                self.bn2 = nn.BatchNorm2d(gf_dim*2)\r\n",
        "                self.relu2 = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "                self.convTrans3 = nn.ConvTranspose2d(gf_dim*2, gf_dim, 4, 2, 1, bias=False)\r\n",
        "                self.bn3 = nn.BatchNorm2d(gf_dim)\r\n",
        "                self.relu3 = nn.ReLU(inplace=True)\r\n",
        "\r\n",
        "                self.convTrans4 = nn.ConvTranspose2d(gf_dim, c_dim, 4, 2, 1, bias=False)\r\n",
        "                self.tanh = nn.Tanh()\r\n",
        "\r\n",
        "\r\n",
        "        def forward(self, z):\r\n",
        "                h0 = self.relu0(self.bn0(self.convTrans0(z)))\r\n",
        "                h1 = self.relu1(self.bn1(self.convTrans1(h0)))\r\n",
        "                h2 = self.relu2(self.bn2(self.convTrans2(h1)))\r\n",
        "                h3 = self.relu3(self.bn3(self.convTrans3(h2)))\r\n",
        "                h4 = self.convTrans4(h3)\r\n",
        "                output = self.tanh(h4)\r\n",
        "                return output # (c_dim, 64, 64)\r\n",
        "\r\n",
        "class _ganLogits(nn.Module):\r\n",
        "    '''\r\n",
        "    Layer of the GAN logits of the discriminator\r\n",
        "    The layer gets class logits as inputs and calculates GAN logits to\r\n",
        "    differentiate real and fake images in a numerical stable way\r\n",
        "    '''\r\n",
        "    def __init__(self, num_classes):\r\n",
        "        '''\r\n",
        "        :param num_classes: Number of real data classes (10 for SVHN)\r\n",
        "        '''\r\n",
        "        super(_ganLogits, self).__init__()\r\n",
        "        self.num_classes = num_classes\r\n",
        "\r\n",
        "    def forward(self, class_logits):\r\n",
        "        '''\r\n",
        "        :param class_logits: Unscaled log probabilities of house numbers\r\n",
        "        '''\r\n",
        "\r\n",
        "        # Set gan_logits such that P(input is real | input) = sigmoid(gan_logits).\r\n",
        "        # Keep in mind that class_logits gives you the probability distribution over all the real\r\n",
        "        # classes and the fake class. You need to work out how to transform this multiclass softmax\r\n",
        "        # distribution into a binary real-vs-fake decision that can be described with a sigmoid.\r\n",
        "        # Numerical stability is very important.\r\n",
        "        # You'll probably need to use this numerical stability trick:\r\n",
        "        # log sum_i exp a_i = m + log sum_i exp(a_i - m).\r\n",
        "        # This is numerically stable when m = max_i a_i.\r\n",
        "        # (It helps to think about what goes wrong when...\r\n",
        "        #   1. One value of a_i is very large\r\n",
        "        #   2. All the values of a_i are very negative\r\n",
        "        # This trick and this value of m fix both those cases, but the naive implementation and\r\n",
        "        # other values of m encounter various problems)\r\n",
        "        real_class_logits, fake_class_logits = torch.split(class_logits, self.num_classes, dim=1)\r\n",
        "        fake_class_logits = torch.squeeze(fake_class_logits)\r\n",
        "\r\n",
        "        max_val, _ = torch.max(real_class_logits, 1, keepdim=True)\r\n",
        "        stable_class_logits = real_class_logits - max_val\r\n",
        "        max_val = torch.squeeze(max_val)\r\n",
        "        gan_logits = torch.log(torch.sum(torch.exp(stable_class_logits), 1)) + max_val - fake_class_logits\r\n",
        "\r\n",
        "        return gan_logits\t# [128]\r\n",
        "\r\n",
        "class Discriminator(nn.Module):\r\n",
        "        def __init__(self, z_dim, c_dim, df_dim, class_num):\r\n",
        "                super(Discriminator, self).__init__()\r\n",
        "                self.df_dim = df_dim\r\n",
        "\r\n",
        "                self.conv0 = nn.Conv2d(c_dim, df_dim, 4, 2, 1, bias=False)\r\n",
        "                self.relu0 = nn.LeakyReLU(0.2, inplace=True)\r\n",
        "\r\n",
        "                self.conv1 = nn.Conv2d(df_dim, df_dim*2, 4, 2, 1, bias=False)\r\n",
        "                self.bn1 = nn.BatchNorm2d(df_dim*2)\r\n",
        "                self.relu1 = nn.LeakyReLU(0.2, inplace=True)\r\n",
        "\r\n",
        "                self.conv2 = nn.Conv2d(df_dim*2, df_dim*4, 4, 2, 1, bias=False)\r\n",
        "                self.bn2 = nn.BatchNorm2d(df_dim*4)\r\n",
        "                self.relu2 = nn.LeakyReLU(0.2, inplace=True)\r\n",
        "\r\n",
        "                self.conv3 = nn.Conv2d(df_dim*4, df_dim*8, 4, 2, 1, bias=False)\r\n",
        "                self.bn3 = nn.BatchNorm2d(df_dim*8)\r\n",
        "                self.relu3 = nn.LeakyReLU(0.2, inplace=True)\r\n",
        "\r\n",
        "\t\t#self.fc_z = nn.Linear(df_dim*8*4*4, z_dim)\r\n",
        "                self.fc_aux = nn.Linear(df_dim*8*4*4, class_num+1)\r\n",
        "                self.softmax = nn.LogSoftmax()\r\n",
        "\r\n",
        "                for m in self.modules():\r\n",
        "                        if isinstance(m, nn.Linear):\r\n",
        "                                m.weight.data.normal_(0.0, 0.02)\r\n",
        "                                if m.bias is not None:\r\n",
        "                                        m.bias.data.zero_()\r\n",
        "\r\n",
        "\r\n",
        "        def forward(self, input):\r\n",
        "                h0 = self.relu0(self.conv0(input))\r\n",
        "                h1 = self.relu1(self.bn1(self.conv1(h0)))\r\n",
        "                h2 = self.relu2(self.bn2(self.conv2(h1)))\r\n",
        "                h3 = self.relu3(self.bn3(self.conv3(h2)))\r\n",
        "                #cl = self.class_logistics(h3.view(-1, self.df_dim*8*4*4))\r\n",
        "                #gl = self.gan_logistics(cl)    \r\n",
        "                output = self.softmax(self.fc_aux(h3.view(-1, self.df_dim*8*4*4)))\r\n",
        "                return h3, output\r\n",
        "                #return h3, output.view(-1,1).squeeze(1) # by squeeze, get just float not float Tenosor"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRU147caRRFw"
      },
      "source": [
        "def conditional_latent_generator(distribution, class_num, batch):\r\n",
        "\tclass_labels = torch.randint(0, class_num, (batch,), dtype=torch.long)\r\n",
        "\tfake_z = distribution[class_labels[0].item()].sample((1,))\r\n",
        "\tfor c in class_labels[1:]:\r\n",
        "\t\tfake_z = torch.cat((fake_z, distribution[c.item()].sample((1,))), dim=0)\r\n",
        "\treturn fake_z, class_labels"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFjMloLlRgZk"
      },
      "source": [
        "def batch2one(Z, y, z, class_num):\r\n",
        "\tfor i in range(y.shape[0]):\r\n",
        "\t\tZ[y[i]] = torch.cat((Z[y[i]], z[i].cpu()), dim=0) # Z[label][0] should be deleted..\r\n",
        "\treturn Z\t\t\t\r\n",
        "\t\r\n",
        "class AverageMeter(object):\r\n",
        "    \"\"\" Computes ans stores the average and current value\"\"\"\r\n",
        "    def __init__(self):\r\n",
        "        self.reset()\r\n",
        "    \r\n",
        "    def reset(self):\r\n",
        "        self.val = 0.\r\n",
        "        self.avg = 0.\r\n",
        "        self.sum = 0.\r\n",
        "        self.count = 0\r\n",
        "    \r\n",
        "    def update(self, val, n=1):\r\n",
        "        self.val = val\r\n",
        "        self.sum += val * n\r\n",
        "        self.count += n\r\n",
        "        self.avg = self.sum / self.count\r\n",
        "\r\n",
        "def one_hot(x, num_classes):\r\n",
        "        '''\r\n",
        "        One-hot encoding of the vector of classes. It uses number of classes + 1 to\r\n",
        "        encode fake images\r\n",
        "        :param x: vector of output classes to one-hot encode\r\n",
        "        :return: one-hot encoded version of the input vector\r\n",
        "        '''\r\n",
        "        label_numpy = x.data.cpu().numpy()\r\n",
        "        label_onehot = np.zeros((label_numpy.shape[0], num_classes + 1))\r\n",
        "        label_onehot[np.arange(label_numpy.shape[0]), label_numpy] = 1\r\n",
        "        return torch.FloatTensor(label_onehot)\r\n",
        "\r\n",
        "\r\n",
        "def weights_init(m):\r\n",
        "        classname = m.__class__.__name__\r\n",
        "        if classname.find('Conv') != -1:\r\n",
        "                m.weight.data.normal_(0.0, 0.02)\r\n",
        "        elif classname.find('BatchNorm') != -1:\r\n",
        "                m.weight.data.normal_(1.0, 0.02)\r\n",
        "                m.bias.data.fill_(0)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCst51DRRkkW"
      },
      "source": [
        "def save_checkpoint(state, filename='checkpoint'):\r\n",
        "    torch.save(state, filename + '.pth.tar')\r\n",
        "\r\n",
        "def print_gan_log(epoch, epoches, iteration, iters, learning_rate,\r\n",
        "              display, batch_time, data_time, D_losses, G_losses):\r\n",
        "    print('epoch: [{}/{}] iteration: [{}/{}]\\t'\r\n",
        "          'Learning rate: {}'.format(epoch, epoches, iteration, iters, learning_rate))\r\n",
        "    print('Time {batch_time.sum:.3f}s / {0}iters, ({batch_time.avg:.3f})\\t'\r\n",
        "          'Data load {data_time.sum:.3f}s / {0}iters, ({data_time.avg:3f})\\n'\r\n",
        "          'Loss_D = {loss_D.val:.8f} (ave = {loss_D.avg:.8f})\\n'\r\n",
        "          'Loss_G = {loss_G.val:.8f} (ave = {loss_G.avg:.8f})\\n'.format(\r\n",
        "              display, batch_time=batch_time,\r\n",
        "              data_time=data_time, loss_D=D_losses, loss_G=G_losses))"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocAcgo24RxDa"
      },
      "source": [
        "#Clear\r\n",
        "def print_vae_log(epoch, epoches, iteration, iters, learning_rate,\r\n",
        "              display, batch_time, data_time, losses):\r\n",
        "\r\n",
        "    print('epoch: [{}/{}] iteration: [{}/{}]\\t'\r\n",
        "          'Learning rate: {}'.format(epoch, epoches, iteration, iters, learning_rate))\r\n",
        "    print('Time {batch_time.sum:.3f}s / {0}iters, ({batch_time.avg:.3f})\\t'\r\n",
        "          'Data load {data_time.sum:.3f}s / {0}iters, ({data_time.avg:3f})\\n'\r\n",
        "          'Loss = {loss.val:.8f} (ave = {loss.avg:.8f})\\n'.format(\r\n",
        "              display, batch_time=batch_time,\r\n",
        "              data_time=data_time, loss=losses))"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXPZrvD6Rz9T"
      },
      "source": [
        "def plot_result2(fake, image_size, num_epoch, save_dir, name, fig_size=(8, 8), is_gray=False):\r\n",
        "\r\n",
        "    generate_images = fake\r\n",
        "    #G.train() # for next train after plot_result at a epoch ...\r\n",
        "\r\n",
        "    n_rows = n_cols = 8\r\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=fig_size)\r\n",
        "\r\n",
        "    for ax, img in zip(axes.flatten(), generate_images):\r\n",
        "        ax.axis('off')\r\n",
        "        ax.set_adjustable('box')\r\n",
        "        if is_gray:\r\n",
        "            img = img.cpu().data.view(image_size, image_size).numpy()\r\n",
        "            ax.imshow(img, cmap='gray', aspect='equal')\r\n",
        "        else:\r\n",
        "            img = (((img - img.min()) * 255) / (img.max() - img.min())).cpu().data.numpy().transpose(1, 2, 0).astype(np.uint8)\r\n",
        "            ax.imshow(img, cmap=None, aspect='equal')\r\n",
        "    plt.subplots_adjust(wspace=0, hspace=0)\r\n",
        "    title = 'Epoch {0}'.format(num_epoch)\r\n",
        "    fig.text(0.5, 0.04, title, ha='center')\r\n",
        "\r\n",
        "    if name == \"dcgan\":\r\n",
        "        plt.savefig(os.path.join(save_dir, 'DCGAN_epoch_{}.png'.format(num_epoch)))\r\n",
        "        plt.close()\r\n",
        "\r\n",
        "    elif name == \"anomaly\":\r\n",
        "        plt.savefig(os.path.join(save_dir, 'anoGAN_epoch_{}.png'.format(num_epoch)))\r\n",
        "        plt.close()\r\n",
        "\r\n",
        "    elif name == \"vae\":\r\n",
        "        plt.savefig(os.path.join(save_dir, 'vae_epoch_{}.png'.format(num_epoch)))\r\n",
        "        plt.close()\r\n",
        "\r\n",
        "    elif name ==\"gan\":\r\n",
        "        plt.savefig(os.path.join(save_dir, 'gan_epoch_{}.png'.format(num_epoch)))\r\n",
        "        plt.close()\r\n",
        "\r\n",
        "def plot_result(G, fixed_noise, image_size, num_epoch, save_dir, name, fig_size=(8, 8), is_gray=False):\r\n",
        "\r\n",
        "    G.eval()\r\n",
        "    generate_images = G(fixed_noise)\r\n",
        "    G.train() # for next train after plot_result at a epoch ... \r\n",
        "    \r\n",
        "    n_rows = n_cols = 8\r\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=fig_size)\r\n",
        "    \r\n",
        "    for ax, img in zip(axes.flatten(), generate_images):\r\n",
        "        ax.axis('off')\r\n",
        "        ax.set_adjustable('box')\r\n",
        "        if is_gray:\r\n",
        "            img = img.cpu().data.view(image_size, image_size).numpy()\r\n",
        "            ax.imshow(img, cmap='gray', aspect='equal')\r\n",
        "        else:\r\n",
        "            img = (((img - img.min()) * 255) / (img.max() - img.min())).cpu().data.numpy().transpose(1, 2, 0).astype(np.uint8)\r\n",
        "            ax.imshow(img, cmap=None, aspect='equal')\r\n",
        "    plt.subplots_adjust(wspace=0, hspace=0)\r\n",
        "    title = 'Epoch {0}'.format(num_epoch)\r\n",
        "    fig.text(0.5, 0.04, title, ha='center')\r\n",
        "    \r\n",
        "    if name == \"dcgan\":\r\n",
        "        plt.savefig(os.path.join(save_dir, 'DCGAN_epoch_{}.png'.format(num_epoch)))\r\n",
        "        plt.close()\r\n",
        "\r\n",
        "    elif name == \"anomaly\":\r\n",
        "        plt.savefig(os.path.join(save_dir, 'anoGAN_epoch_{}.png'.format(num_epoch)))\r\n",
        "        plt.close()\r\n",
        "\r\n",
        "    elif name == \"vae\":\r\n",
        "        plt.savefig(os.path.join(save_dir, 'vae_epoch_{}.png'.format(num_epoch)))\r\n",
        "        plt.close()\r\n",
        "    \r\n",
        "    elif name ==\"gan\":\r\n",
        "        plt.savefig(os.path.join(save_dir, 'gan_epoch_{}.png'.format(num_epoch)))\r\n",
        "        plt.close()\r\n",
        "\r\n",
        "#Clear    \r\n",
        "def plot_loss(num_epoch, epoches, save_dir, **loss):\r\n",
        "    fig, ax = plt.subplots() \r\n",
        "    ax.set_xlim(0,epoches + 1)\r\n",
        "    if len(loss) == 2:\r\n",
        "        ax.set_ylim(0, max(np.max(loss['g_loss']), np.max(loss['d_loss'])) * 1.1)\r\n",
        "    elif len(loss) == 1:\r\n",
        "        ax.set_ylim(0, max(np.max(loss['vae_loss'])) * 1.1)\r\n",
        "    plt.xlabel('Epoch {}'.format(num_epoch))\r\n",
        "    plt.ylabel('Loss')\r\n",
        "    \r\n",
        "    if len(loss) == 2:\r\n",
        "        plt.plot([i for i in range(1, num_epoch + 1)], loss['d_loss'], label='Discriminator', color='red', linewidth=3)\r\n",
        "        plt.plot([i for i in range(1, num_epoch + 1)], loss['g_loss'], label='Generator', color='mediumblue', linewidth=3)\r\n",
        "        plt.legend()\r\n",
        "        plt.savefig(os.path.join(os.path.join(save_dir, \"loss\"), 'gan_loss_epoch_{}.png'.format(num_epoch)))\r\n",
        "    elif len(loss) == 1:\r\n",
        "        plt.plot([i for i in range(1, num_epoch + 1)], loss['vae_loss'], label='vae_loss', color='red', linewidht=3)\r\n",
        "        plt.legend()\r\n",
        "        plt.savefig(os.path.join(os.path.join(save_dir, \"loss\"), 'vae_loss_epoch_{}.png'.format(num_epoch)))\r\n",
        " \r\n",
        "    plt.close()\r\n",
        "\r\n",
        "#Clear\r\n",
        "def plot_accuracy(num_epoch, epoches, save_dir, real_acc, fake_acc):\r\n",
        "\tfig, ax = plt.subplots()\r\n",
        "\tax.set_xlim(0,epoches + 1)\r\n",
        "\tax.set_ylim(0, max(np.max(real_acc), np.max(fake_acc)) * 1.1)\r\n",
        "\tplt.xlabel('Epoch {}'.format(num_epoch))\r\n",
        "\tplt.ylabel('Accuracy')\r\n",
        "\r\n",
        "\tplt.plot([i for i in range(1, num_epoch + 1)], real_acc, label='real data', color='red', linewidth=3)\r\n",
        "\tplt.plot([i for i in range(1, num_epoch + 1)], fake_acc, label='fake data', color='mediumblue', linewidth=3)\r\n",
        "\tplt.legend()\r\n",
        "\tplt.savefig(os.path.join(os.path.join(save_dir, \"accuracy\"), 'gan_accuracy_epoch_{}.png'.format(num_epoch)))\r\n",
        "\r\n",
        "\tplt.close()"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7sEvv5gSer1"
      },
      "source": [
        "import torch.distributions.multivariate_normal as mn"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCdghNksVggK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01fea7e1-31d3-42d8-cef7-cb5f8143031f"
      },
      "source": [
        "pretrained = True\r\n",
        "if __name__ == \"__main__\":\r\n",
        "  use_cuda = torch.cuda.is_available()\r\n",
        "  gpu = 0\r\n",
        "\r\n",
        "  train_loader = load_images(batch_size, image_size)\r\n",
        "\r\n",
        "\r\n",
        "  decoder = Decoder(z_dim, c_dim, gf_dim)\r\n",
        "  encoder = Encoder(z_dim, c_dim, df_dim)\r\n",
        "\r\n",
        "  if not pretrained:\r\n",
        "    if use_cuda:\r\n",
        "      decoder = decoder.cuda(gpu)\r\n",
        "      encoder = encoder.cuda(gpu)\r\n",
        "\r\n",
        "    # WHY BECLoss() - only need to determine fake/real for Discriminator\r\n",
        "    criterion = nn.BCELoss()\r\n",
        "    if use_cuda:\r\n",
        "      criterion = criterion.cuda(gpu)\r\n",
        "\r\n",
        "    optimizerE = torch.optim.Adam(encoder.parameters(), lr=base_lr, betas=(beta1, 0.999))\r\n",
        "    optimizerD = torch.optim.Adam(decoder.parameters(), lr=base_lr, betas=(beta1, 0.999))\r\n",
        "\r\n",
        "    batch_time = AverageMeter()\r\n",
        "    data_time = AverageMeter()\r\n",
        "    losses = AverageMeter()\r\n",
        "\r\n",
        "    fixed_noise = torch.FloatTensor(8 * 8, z_dim, 1, 1).normal_(0, 1)\r\n",
        "    if use_cuda:\r\n",
        "      fixed_noise = fixed_noise.cuda(gpu)\r\n",
        "    with torch.no_grad():\r\n",
        "      fixed_noisev = fixed_noise\r\n",
        "\r\n",
        "    end = time.time()\r\n",
        "    \r\n",
        "    encoder.train()\r\n",
        "    decoder.train()\r\n",
        "    loss_list = []\r\n",
        "\r\n",
        "    \r\n",
        "    criterion = nn.MSELoss(size_average=False)\t\r\n",
        "    for epoch in range(epoches):\r\n",
        "      for i, (input, label) in enumerate(train_loader):\r\n",
        "        #l Update 'D' : max log(D(x)) + log(1-D(G(z)))\r\n",
        "        data_time.update(time.time()-end)\r\n",
        "      \r\n",
        "        batch_size = input.size(0)\r\n",
        "        if use_cuda:\r\n",
        "          input = input.cuda(gpu)\r\n",
        "        \r\n",
        "        mu, log_sigmoid = encoder(input)\r\n",
        "        # reparameterization\r\n",
        "        std = torch.exp(log_sigmoid/2)\r\n",
        "        eps = torch.randn_like(std)\r\n",
        "        z = mu + eps * std\r\n",
        "        z = z.view(-1, z_dim, 1, 1)\t\r\n",
        "        if use_cuda:\r\n",
        "          z = z.cuda(gpu)\r\n",
        "\r\n",
        "        # reconstruct image\r\n",
        "        x_reconstruct = decoder(z)\r\n",
        "\r\n",
        "        # reconstruct_loss + KL_divergence\r\n",
        "        reconstruct_loss = criterion(x_reconstruct, input)\r\n",
        "        kl_div = -0.5 * torch.sum(1+log_sigmoid-mu.pow(2)-log_sigmoid.exp())\r\n",
        "        loss = reconstruct_loss + kl_div\r\n",
        "        losses.update(loss.item())\t\r\n",
        "        optimizerE.zero_grad()\r\n",
        "        optimizerD.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        optimizerE.step()\r\n",
        "        optimizerD.step()\r\n",
        "\r\n",
        "        batch_time.update(time.time()-end)\r\n",
        "        end = time.time()\r\n",
        "    \r\n",
        "        # log every 100th train data of train_loader - display(100)\t\r\n",
        "        if (i+1) % display == 0:\r\n",
        "          print_vae_log(epoch+1, epoches, i+1, len(train_loader), base_lr, display, batch_time, data_time, losses)\r\n",
        "          # Is it Continous ???\r\n",
        "          batch_time.reset()\r\n",
        "          data_time.reset()\r\n",
        "        # log every 1 epoch (all of train_loader)\r\n",
        "        elif (i+1) == len(train_loader):\r\n",
        "          print_vae_log(epoch + 1, epoches, i + 1, len(train_loader), base_lr, (i + 1) % display, batch_time, data_time, losses)\r\n",
        "          batch_time.reset()\r\n",
        "          data_time.reset()\r\n",
        "\r\n",
        "      # log every 1 epoch\r\n",
        "      loss_list.append(losses.avg)\r\n",
        "      losses.reset()\r\n",
        "\r\n",
        "      plot_result(decoder, fixed_noisev, image_size, epoch + 1,save_dir, 'vae', is_gray=(c_dim == 1))\r\n",
        "      #plot_loss(epoch+1, config.epoches, args.save_dir, vae_loss=loss_list)\r\n",
        "      # save the D and G.\r\n",
        "      save_checkpoint({'epoch': epoch, 'state_dict': encoder.state_dict(),}, os.path.join(os.path.join(checkpoint_dir,\"vae\"), 'encoder_epoch_{}'.format(epoch)))\r\n",
        "      save_checkpoint({'epoch': epoch, 'state_dict': decoder.state_dict(),}, os.path.join(os.path.join(checkpoint_dir,\"vae\"), 'decoder_epoch_{}'.format(epoch)))\r\n",
        "\r\n",
        "    #create_gif(epoches, save_dir, 'vae')\r\n",
        "\r\n",
        "  ## Class Conditional Generator - Pretrained Model\"\r\n",
        "  else:\r\n",
        "    print(\"Class Conditional Generator - Use Pretrained Model\")\r\n",
        "    if use_cuda:\r\n",
        "      encoder = encoder.cuda(gpu)\r\n",
        "      decoder = decoder.cuda(gpu)\r\n",
        "    encoder.load_state_dict(torch.load(os.path.join(os.path.join(checkpoint_dir, \"vae\"), \"encoder_epoch_\"+ str(epoches-1) + \".pth.tar\"))['state_dict'])\r\n",
        "    decoder.load_state_dict(torch.load(os.path.join(os.path.join(checkpoint_dir, \"vae\"), \"decoder_epoch_\"+ str(epoches-1) + \".pth.tar\"))['state_dict'])\r\n",
        "    #Z = np.empty([config.class_num, config.z_dim], dtype=float)\r\n",
        "    # Z : [label-1, labe-2, ... ]\r\n",
        "    # Z[label-1] : [[z1], [z2], ... ] (#labeld_data, #z_dim)\r\n",
        "    encoder.eval()\r\n",
        "    decoder.eval()\r\n",
        "    Z = []\r\n",
        "    with torch.no_grad():\r\n",
        "      for i in range(class_num):\r\n",
        "        Z.append(torch.zeros((1, z_dim), dtype=torch.float)) # Z : [class_num, z_dim]\r\n",
        "    \r\n",
        "      for i, (input, label) in enumerate(train_loader):\r\n",
        "        if use_cuda:\r\n",
        "          input = input.cuda(gpu)\r\n",
        "        mu, log_sigmoid = encoder(input)\r\n",
        "        std = torch.exp(log_sigmoid/2)\r\n",
        "        eps = torch.randn_like(std)\r\n",
        "        z = mu + eps * std\r\n",
        "        z = z.view(-1, 1, z_dim)\r\n",
        "        Z = batch2one(Z, label, z, class_num)\r\n",
        "\r\n",
        "      N = []\r\n",
        "      for i in range(class_num):\r\n",
        "        label_mean = torch.mean(Z[i][1:], dim=0).float()\r\n",
        "        label_cov = torch.from_numpy(np.cov(Z[i][1:].numpy(), rowvar=False)).float()\r\n",
        "        #print(\"{}th Z : {}\".format(i+1, Z[i][1:].shape))\r\n",
        "        #print(\"{}-class  mean : {}\".format(i+1, label_mean.shape))\r\n",
        "        #print(\"{}-class covariance : {}\".format(i+1, label_cov.shape))\r\n",
        "        m = mn.MultivariateNormal(label_mean, label_cov)\r\n",
        "        sample = m.sample((64,))\r\n",
        "        print(sample.shape)\r\n",
        "        if use_cuda:\r\n",
        "          sample = sample.cuda(gpu)\r\n",
        "        fake = decoder(sample.view(-1, z_dim, 1, 1))\r\n",
        "        \r\n",
        "        \r\n",
        "        plot_result2(fake, image_size, i, 'data/gan/samples', 'ssgan', is_gray=(c_dim == 1))\r\n",
        "        N.append(m)\r\n",
        "      print(\"uhm\")\r\n",
        "      torch.save({'distribution': N}, os.path.join(distribution_dir, 'class_distribution')+'.dt')"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class Conditional Generator - Use Pretrained Model\n",
            "torch.Size([64, 100])\n",
            "torch.Size([64, 100])\n",
            "uhm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58SE8dlOTKJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa8f5da-7b8d-4e50-9bfd-f6e13e102f58"
      },
      "source": [
        "pretrained = True\r\n",
        "if __name__ == \"__main__\":\r\n",
        "  use_cuda = torch.cuda.is_available()\r\n",
        "  gpu = 0\r\n",
        "\r\n",
        "  # parser.add_argument('--save_dir', type=str, dest='save_dir', help='the path of generated data dir', default='sample')\r\n",
        "  # parser.add_argument('--distribution_dir', type=str, dest='distribution_dir', help='the path of class distribution dir', default='distribution')\r\n",
        "  # parser.add_argument('--test_dir', type=str, dest='test_dir', help='the path of anomaly test data')\r\n",
        "  # parser.add_argument('--test_result_dir', type=str, dest='test_result_dir', help='the path of anomaly test result dir')\r\n",
        "  train_loader = load_images(batch_size, image_size)\r\n",
        "\r\n",
        "  G = Generator(z_dim, c_dim, gf_dim)\r\n",
        "  G.apply(weights_init)\r\n",
        "\r\n",
        "  D = Discriminator(z_dim, c_dim, df_dim, class_num)\r\n",
        "  D.apply(weights_init)\r\n",
        "\r\n",
        "  # if not pretrained:\r\n",
        "  #   if use_cuda:\r\n",
        "  #     print(\"duh\")\r\n",
        "  #     G = G.cuda(gpu)\r\n",
        "  #     D = D.cuda(gpu)\r\n",
        "\r\n",
        "  if use_cuda:\r\n",
        "    print(\"duh\")\r\n",
        "    G = G.cuda(gpu)\r\n",
        "    D = D.cuda(gpu)\r\n",
        "\r\n",
        "\r\n",
        "  distribution = torch.load(os.path.join(distribution_dir,'class_distribution.dt'))['distribution']\r\n",
        "\r\n",
        "  #G.load_state_dict(torch.load(os.path.join(os.path.join(args.checkpoint_dir,\"vae\") , \"decoder_epoch_\"+str(config.epoches-1) + \".pth.tar\"))['state_dict'])\r\n",
        "\r\n",
        "  # state_e = torch.load(os.path.join(os.path.join(args.checkpoint_dir,\"vae\"), \"encoder_epoch_\"+str(config.epoches-1) + \".pth.tar\"))['state_dict']\r\n",
        "  # del state_e['fc_z1.weight']\r\n",
        "  # del state_e['fc_z1.bias']\r\n",
        "  # del state_e['fc_z2.weight']\r\n",
        "  # del state_e['fc_z2.bias']\r\n",
        "  # state_e.update({'fc_aux.weight':D.state_dict()['fc_aux.weight']})\r\n",
        "  # state_e.update({'fc_aux.bias':D.state_dict()['fc_aux.bias']})\r\n",
        "\r\n",
        "  # D.load_state_dict(state_e)\r\n",
        "\r\n",
        "  criterion = nn.NLLLoss()\r\n",
        "\r\n",
        "  optimizerD = torch.optim.Adam(D.parameters(), lr=base_lr, betas=(beta1, 0.999))\r\n",
        "  optimizerG = torch.optim.Adam(G.parameters(), lr=base_lr, betas=(beta1, 0.999))\r\n",
        "\r\n",
        "  batch_time = AverageMeter()\r\n",
        "  data_time = AverageMeter()\r\n",
        "  D_losses = AverageMeter()\r\n",
        "  G_losses = AverageMeter()\r\n",
        "\r\n",
        "  #fixed_noise = torch.FloatTensor(8 * 8, config.z_dim, 1, 1).normal_(0, 1)\r\n",
        "  #if use_cuda:\r\n",
        "  #\tfixed_noise = fixed_noise.cuda(gpu)\r\n",
        "  #with torch.no_grad():\r\n",
        "  #\tfixed_noisev = fixed_noise\r\n",
        "\r\n",
        "  end = time.time()\r\n",
        "\r\n",
        "  D.train()\r\n",
        "  G.train()\r\n",
        "  D_loss_list = []\r\n",
        "  G_loss_list = []\r\n",
        "\r\n",
        "  real_label = torch.LongTensor(batch_size)\r\n",
        "  fake_label = torch.LongTensor(batch_size)\t\r\n",
        "\r\n",
        "  for epoch in range(epoches):\r\n",
        "    total_real = 0\r\n",
        "    total_fake = 0\r\n",
        "    correct_real = 0\r\n",
        "    correct_fake = 0\r\n",
        "    for i, (input, label) in enumerate(train_loader):\r\n",
        "      # Update 'D' : max log(D(x)) + log(1-D(G(z)))\r\n",
        "      data_time.update(time.time()-end)\r\n",
        "      batch_size = input.size(0)\r\n",
        "      fake_num = math.ceil(batch_size/class_num)\t# For each batch, 1/(n+1) of total images are fake\r\n",
        "      conditional_z, z_label = conditional_latent_generator(distribution, class_num, batch_size)\r\n",
        "\r\n",
        "      label = label.long().squeeze() # \"squeeze\" : [batch, 1] --> [batch] ... e.g) [1,2,3,4...]\t\t\r\n",
        "      print(label[0])\r\n",
        "      #print(path[0])\r\n",
        "      if use_cuda:\r\n",
        "        input = input.cuda(gpu)\r\n",
        "        label = label.cuda(gpu)\r\n",
        "\r\n",
        "      sample_features, D_real = D(input)\r\n",
        "      real_label.resize_(batch_size).copy_(label)\t# \"cpu\" : gpu --> cpu // <<.data.cpu vs cpu>> // \"resize_as\" : get tensor size and resize \r\n",
        "      if use_cuda:\r\n",
        "        real_label = real_label.cuda(gpu) \r\n",
        "      \r\n",
        "      D_loss_real = criterion(D_real, real_label)\r\n",
        "      noise = conditional_z[0:fake_num].view(-1, z_dim, 1, 1)\r\n",
        "\r\n",
        "      fake_label.resize_(noise.shape[0]).fill_(class_num)\t# fake_label = '(num_class)+1'\r\n",
        "      if use_cuda:\r\n",
        "        noise = noise.cuda(gpu)\r\n",
        "        fake_label = fake_label.cuda(gpu)\r\n",
        "        z_label = z_label.cuda(gpu)\r\n",
        "      \r\n",
        "      fake = G(noise)\r\n",
        "\r\n",
        "      _, D_fake = D(fake.detach())\t# Fake image...\r\n",
        "      D_loss_fake = criterion(D_fake, fake_label)\t# Hmmmm...... fake_label? or z_label?\r\n",
        "\r\n",
        "      D_loss = D_loss_real + D_loss_fake\r\n",
        "      D_losses.update(D_loss.item())\r\n",
        "      D.zero_grad()\r\n",
        "      G.zero_grad()\r\n",
        "      D_loss.backward()\r\n",
        "      optimizerD.step()\r\n",
        "\r\n",
        "      # Update 'G' : max log(D(G(z)))\r\n",
        "      noise = conditional_z.view(-1, z_dim, 1, 1)\r\n",
        "      if use_cuda:\r\n",
        "        noise = noise.cuda(gpu)\r\n",
        "      fake = G(noise)\r\n",
        "      _, D_fake = D(fake)\r\n",
        "      G_loss = criterion(D_fake, z_label)\r\n",
        "      #G_losses.update(G_loss.data[0])\r\n",
        "      G_losses.update(G_loss.item())\r\n",
        "\r\n",
        "      D.zero_grad()\r\n",
        "      G.zero_grad()\r\n",
        "      G_loss.backward()\r\n",
        "      optimizerG.step()\r\n",
        "\r\n",
        "      batch_time.update(time.time()-end)\r\n",
        "      end = time.time()\r\n",
        "      \r\n",
        "      pred_real = torch.max(D_real.data, 1)[1]\r\n",
        "      pred_fake = torch.max(D_fake.data, 1)[1]\r\n",
        "      total_real += real_label.size(0)\r\n",
        "      total_fake += z_label.size(0)\r\n",
        "      correct_real += (pred_real == real_label).sum().item()\r\n",
        "      correct_fake += (pred_fake == z_label).sum().item()\r\n",
        "\r\n",
        "      # log every 100th train data of train_loader - display(100)\t\r\n",
        "      if (i+1) % display == 0:\r\n",
        "        print_gan_log(epoch+1, epoches, i+1, len(train_loader), base_lr, display, batch_time, data_time, D_losses, G_losses)\r\n",
        "        # Is it Continous ???\r\n",
        "        batch_time.reset()\r\n",
        "        data_time.reset()\r\n",
        "      # log every 1 epoch (all of train_loader) ... \"End of all mini-Batch\"\r\n",
        "      elif (i+1) == len(train_loader):\r\n",
        "        print_gan_log(epoch + 1, epoches, i + 1, len(train_loader), base_lr,\r\n",
        "                          (i + 1) % display, batch_time, data_time, D_losses, G_losses)\r\n",
        "        #accuracy = masked_correct.item()/max(1.0, num_samples.item())\r\n",
        "        plot_result2(fake, image_size, epoch + 1, save_dir, 'gan', is_gray=(c_dim == 1))\r\n",
        "        real_acc = 100 * correct_real / total_real\r\n",
        "        fake_acc = 100 * correct_fake / total_fake\r\n",
        "        print('Real Accuracy : {}'.format(real_acc))\r\n",
        "        print('Fake Accuracy : {}'.format(fake_acc))\r\n",
        "        #plot_accuracy(epoch+1, epoches, save_dir, real_acc, fake_acc)\r\n",
        "        batch_time.reset()\r\n",
        "        data_time.reset()\r\n",
        "\r\n",
        "    # log every 1 epoch\r\n",
        "    D_loss_list.append(D_losses.avg)\r\n",
        "    G_loss_list.append(G_losses.avg)\r\n",
        "    D_losses.reset()\r\n",
        "    G_losses.reset()\r\n",
        "\r\n",
        "    plot_result(G, fixed_noisev, image_size, epoch + 1, save_dir, 'gan', is_gray=(c_dim == 1))\r\n",
        "    plot_loss(epoch+1,epoches, save_dir, d_loss=D_loss_list, g_loss=G_loss_list)\r\n",
        "    # save the D and G.\r\n",
        "    save_checkpoint({'epoch': epoch, 'state_dict': D.state_dict(),}, os.path.join(os.path.join(checkpoint_dir,'gan'), 'D_epoch_{}'.format(epoch)))\r\n",
        "    save_checkpoint({'epoch': epoch, 'state_dict': G.state_dict(),}, os.path.join(os.path.join(checkpoint_dir,'gan'), 'G_epoch_{}'.format(epoch)))\r\n"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "duh\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:198: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [1/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 1.132s / 10iters, (0.113)\tData load 0.533s / 10iters, (0.053338)\n",
            "Loss_D = 1.66801262 (ave = 2.13104272)\n",
            "Loss_G = 11.38130093 (ave = 8.34327292)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [1/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.655s / 10iters, (0.065)\tData load 0.069s / 10iters, (0.006884)\n",
            "Loss_D = 2.56771088 (ave = 1.99893191)\n",
            "Loss_G = 11.26569080 (ave = 8.81776841)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [1/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.475s / 10iters, (0.047)\tData load 0.061s / 10iters, (0.006075)\n",
            "Loss_D = 1.16412222 (ave = 2.12190839)\n",
            "Loss_G = 8.95618343 (ave = 9.76896586)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [1/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.378s / 10iters, (0.038)\tData load 0.061s / 10iters, (0.006126)\n",
            "Loss_D = 1.89696288 (ave = 2.23980356)\n",
            "Loss_G = 13.04089260 (ave = 10.22265611)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [1/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.159s / 8iters, (0.020)\tData load 0.033s / 8iters, (0.004160)\n",
            "Loss_D = 3.12339592 (ave = 2.19570387)\n",
            "Loss_G = 26.59724045 (ave = 10.83748734)\n",
            "\n",
            "Real Accuracy : 56.23529411764706\n",
            "Fake Accuracy : 0.23529411764705882\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [2/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.046s / 10iters, (0.505)\tData load 4.414s / 10iters, (0.441422)\n",
            "Loss_D = 0.75323427 (ave = 1.86895779)\n",
            "Loss_G = 10.87349892 (ave = 13.34979239)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [2/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.594s / 10iters, (0.059)\tData load 0.044s / 10iters, (0.004351)\n",
            "Loss_D = 0.92919070 (ave = 1.60369420)\n",
            "Loss_G = 10.25919247 (ave = 12.67101834)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [2/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.493s / 10iters, (0.049)\tData load 0.054s / 10iters, (0.005390)\n",
            "Loss_D = 1.50889397 (ave = 1.55607772)\n",
            "Loss_G = 11.53080177 (ave = 13.09969551)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [2/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.375s / 10iters, (0.037)\tData load 0.036s / 10iters, (0.003561)\n",
            "Loss_D = 0.37524498 (ave = 1.68129747)\n",
            "Loss_G = 8.53161526 (ave = 12.98108659)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [2/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.159s / 8iters, (0.020)\tData load 0.032s / 8iters, (0.004048)\n",
            "Loss_D = 6.01877880 (ave = 1.86050109)\n",
            "Loss_G = 15.42994499 (ave = 12.71430755)\n",
            "\n",
            "Real Accuracy : 62.35294117647059\n",
            "Fake Accuracy : 0.23529411764705882\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [3/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 4.957s / 10iters, (0.496)\tData load 4.317s / 10iters, (0.431707)\n",
            "Loss_D = 0.69638813 (ave = 2.13186983)\n",
            "Loss_G = 15.98613644 (ave = 11.06252327)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [3/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.552s / 10iters, (0.055)\tData load 0.062s / 10iters, (0.006232)\n",
            "Loss_D = 1.37923086 (ave = 1.96675592)\n",
            "Loss_G = 5.23879147 (ave = 9.65769737)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [3/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.547s / 10iters, (0.055)\tData load 0.078s / 10iters, (0.007787)\n",
            "Loss_D = 0.32204437 (ave = 1.96019084)\n",
            "Loss_G = 6.01019573 (ave = 9.35509760)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [3/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.386s / 10iters, (0.039)\tData load 0.063s / 10iters, (0.006263)\n",
            "Loss_D = 1.65629840 (ave = 1.83826911)\n",
            "Loss_G = 9.91712093 (ave = 9.39842715)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [3/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.179s / 8iters, (0.022)\tData load 0.036s / 8iters, (0.004537)\n",
            "Loss_D = 3.23055983 (ave = 1.80663672)\n",
            "Loss_G = 3.51047134 (ave = 9.30560881)\n",
            "\n",
            "Real Accuracy : 63.294117647058826\n",
            "Fake Accuracy : 0.9411764705882353\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [4/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.297s / 10iters, (0.530)\tData load 4.749s / 10iters, (0.474858)\n",
            "Loss_D = 0.77794671 (ave = 1.79156220)\n",
            "Loss_G = 8.75146198 (ave = 7.85326653)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [4/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.387s / 10iters, (0.039)\tData load 0.050s / 10iters, (0.005023)\n",
            "Loss_D = 0.91630024 (ave = 1.64758827)\n",
            "Loss_G = 7.51830053 (ave = 7.76580670)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [4/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.489s / 10iters, (0.049)\tData load 0.041s / 10iters, (0.004063)\n",
            "Loss_D = 1.72944486 (ave = 1.64818187)\n",
            "Loss_G = 9.79209423 (ave = 8.26473420)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [4/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.394s / 10iters, (0.039)\tData load 0.060s / 10iters, (0.005981)\n",
            "Loss_D = 1.33016419 (ave = 1.59913054)\n",
            "Loss_G = 6.09944296 (ave = 8.08630275)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [4/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.151s / 8iters, (0.019)\tData load 0.027s / 8iters, (0.003358)\n",
            "Loss_D = 2.94034290 (ave = 1.56816586)\n",
            "Loss_G = 21.95845032 (ave = 8.36052447)\n",
            "\n",
            "Real Accuracy : 64.70588235294117\n",
            "Fake Accuracy : 1.411764705882353\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [5/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.062s / 10iters, (0.506)\tData load 4.429s / 10iters, (0.442886)\n",
            "Loss_D = 1.48566329 (ave = 2.41800752)\n",
            "Loss_G = 9.49310875 (ave = 9.89061885)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [5/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.520s / 10iters, (0.052)\tData load 0.057s / 10iters, (0.005693)\n",
            "Loss_D = 1.89808905 (ave = 1.68841361)\n",
            "Loss_G = 9.57355881 (ave = 8.55834553)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [5/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.404s / 10iters, (0.040)\tData load 0.026s / 10iters, (0.002599)\n",
            "Loss_D = 0.47043595 (ave = 1.57099075)\n",
            "Loss_G = 9.76004410 (ave = 7.93369595)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [5/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.459s / 10iters, (0.046)\tData load 0.075s / 10iters, (0.007496)\n",
            "Loss_D = 1.26496124 (ave = 1.47131525)\n",
            "Loss_G = 7.06401110 (ave = 7.89877000)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [5/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.165s / 8iters, (0.021)\tData load 0.029s / 8iters, (0.003578)\n",
            "Loss_D = 4.60208178 (ave = 1.53039000)\n",
            "Loss_G = 3.71633720 (ave = 7.73489974)\n",
            "\n",
            "Real Accuracy : 65.17647058823529\n",
            "Fake Accuracy : 1.411764705882353\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [6/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.055s / 10iters, (0.506)\tData load 4.504s / 10iters, (0.450446)\n",
            "Loss_D = 1.16347337 (ave = 1.95992948)\n",
            "Loss_G = 10.47252750 (ave = 8.26811223)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [6/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.424s / 10iters, (0.042)\tData load 0.048s / 10iters, (0.004802)\n",
            "Loss_D = 0.68111241 (ave = 1.53290664)\n",
            "Loss_G = 6.31003952 (ave = 7.56231381)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [6/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.462s / 10iters, (0.046)\tData load 0.073s / 10iters, (0.007263)\n",
            "Loss_D = 0.06905064 (ave = 1.39360953)\n",
            "Loss_G = 7.29999447 (ave = 7.38466324)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [6/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.563s / 10iters, (0.056)\tData load 0.080s / 10iters, (0.008004)\n",
            "Loss_D = 0.45042175 (ave = 1.40246736)\n",
            "Loss_G = 11.72521877 (ave = 7.57921147)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [6/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.152s / 8iters, (0.019)\tData load 0.027s / 8iters, (0.003373)\n",
            "Loss_D = 1.35224438 (ave = 1.38602434)\n",
            "Loss_G = 4.62616587 (ave = 7.39360201)\n",
            "\n",
            "Real Accuracy : 69.17647058823529\n",
            "Fake Accuracy : 0.9411764705882353\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [7/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.099s / 10iters, (0.510)\tData load 4.481s / 10iters, (0.448075)\n",
            "Loss_D = 0.82112086 (ave = 1.22919233)\n",
            "Loss_G = 4.26147795 (ave = 7.51701431)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [7/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.482s / 10iters, (0.048)\tData load 0.065s / 10iters, (0.006532)\n",
            "Loss_D = 1.46334147 (ave = 1.13029972)\n",
            "Loss_G = 4.07866573 (ave = 7.37003417)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [7/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.507s / 10iters, (0.051)\tData load 0.049s / 10iters, (0.004877)\n",
            "Loss_D = 2.32092333 (ave = 1.25401375)\n",
            "Loss_G = 10.30544567 (ave = 7.15129205)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [7/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.427s / 10iters, (0.043)\tData load 0.062s / 10iters, (0.006208)\n",
            "Loss_D = 0.65681022 (ave = 1.31378634)\n",
            "Loss_G = 3.56932187 (ave = 6.83918907)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [7/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.156s / 8iters, (0.019)\tData load 0.029s / 8iters, (0.003579)\n",
            "Loss_D = 0.76532769 (ave = 1.33614711)\n",
            "Loss_G = 2.35663223 (ave = 6.58366080)\n",
            "\n",
            "Real Accuracy : 68.23529411764706\n",
            "Fake Accuracy : 0.7058823529411765\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [8/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.178s / 10iters, (0.518)\tData load 4.634s / 10iters, (0.463397)\n",
            "Loss_D = 0.86350739 (ave = 1.59730263)\n",
            "Loss_G = 5.70747328 (ave = 6.03345356)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [8/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.432s / 10iters, (0.043)\tData load 0.058s / 10iters, (0.005846)\n",
            "Loss_D = 1.39068723 (ave = 1.68823155)\n",
            "Loss_G = 7.78906345 (ave = 5.73623447)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [8/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.516s / 10iters, (0.052)\tData load 0.056s / 10iters, (0.005637)\n",
            "Loss_D = 1.65558672 (ave = 1.60618829)\n",
            "Loss_G = 4.62263966 (ave = 5.63699540)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [8/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.416s / 10iters, (0.042)\tData load 0.042s / 10iters, (0.004165)\n",
            "Loss_D = 2.05874944 (ave = 1.54884629)\n",
            "Loss_G = 3.08769560 (ave = 5.55393326)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [8/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.236s / 8iters, (0.030)\tData load 0.023s / 8iters, (0.002841)\n",
            "Loss_D = 1.32358766 (ave = 1.60201786)\n",
            "Loss_G = 15.25515175 (ave = 5.75429840)\n",
            "\n",
            "Real Accuracy : 64.94117647058823\n",
            "Fake Accuracy : 3.5294117647058822\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [9/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.090s / 10iters, (0.509)\tData load 4.537s / 10iters, (0.453703)\n",
            "Loss_D = 0.36196387 (ave = 1.77633975)\n",
            "Loss_G = 3.42178583 (ave = 5.51998212)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [9/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.445s / 10iters, (0.045)\tData load 0.042s / 10iters, (0.004155)\n",
            "Loss_D = 0.19304463 (ave = 1.52007715)\n",
            "Loss_G = 2.87226081 (ave = 5.22467688)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [9/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.572s / 10iters, (0.057)\tData load 0.076s / 10iters, (0.007618)\n",
            "Loss_D = 1.80731082 (ave = 1.47336732)\n",
            "Loss_G = 4.32362890 (ave = 5.01787623)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [9/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.415s / 10iters, (0.041)\tData load 0.044s / 10iters, (0.004435)\n",
            "Loss_D = 1.61807251 (ave = 1.46305160)\n",
            "Loss_G = 6.75872183 (ave = 5.08393379)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [9/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.165s / 8iters, (0.021)\tData load 0.033s / 8iters, (0.004136)\n",
            "Loss_D = 0.42181435 (ave = 1.43833977)\n",
            "Loss_G = 7.20084238 (ave = 5.16194481)\n",
            "\n",
            "Real Accuracy : 65.17647058823529\n",
            "Fake Accuracy : 3.0588235294117645\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [10/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.166s / 10iters, (0.517)\tData load 4.546s / 10iters, (0.454638)\n",
            "Loss_D = 1.26841891 (ave = 1.12107877)\n",
            "Loss_G = 4.59382343 (ave = 4.48739388)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [10/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.473s / 10iters, (0.047)\tData load 0.041s / 10iters, (0.004136)\n",
            "Loss_D = 1.19677949 (ave = 1.11956313)\n",
            "Loss_G = 3.91652727 (ave = 5.00404513)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [10/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.520s / 10iters, (0.052)\tData load 0.071s / 10iters, (0.007148)\n",
            "Loss_D = 2.26603127 (ave = 1.19170248)\n",
            "Loss_G = 2.88276029 (ave = 4.81868087)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [10/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.321s / 10iters, (0.032)\tData load 0.039s / 10iters, (0.003887)\n",
            "Loss_D = 1.38316417 (ave = 1.25099039)\n",
            "Loss_G = 4.08944511 (ave = 4.94018259)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [10/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.151s / 8iters, (0.019)\tData load 0.030s / 8iters, (0.003745)\n",
            "Loss_D = 0.21824680 (ave = 1.19221977)\n",
            "Loss_G = 5.06721544 (ave = 4.85694912)\n",
            "\n",
            "Real Accuracy : 70.58823529411765\n",
            "Fake Accuracy : 4.235294117647059\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [11/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.181s / 10iters, (0.518)\tData load 4.681s / 10iters, (0.468065)\n",
            "Loss_D = 0.25824654 (ave = 1.23986377)\n",
            "Loss_G = 5.34777689 (ave = 4.58496250)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [11/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.487s / 10iters, (0.049)\tData load 0.046s / 10iters, (0.004611)\n",
            "Loss_D = 1.75676835 (ave = 1.28874941)\n",
            "Loss_G = 9.23893738 (ave = 5.15015879)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [11/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.447s / 10iters, (0.045)\tData load 0.046s / 10iters, (0.004636)\n",
            "Loss_D = 1.31550336 (ave = 1.36444568)\n",
            "Loss_G = 6.97823381 (ave = 5.26434993)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [11/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.420s / 10iters, (0.042)\tData load 0.071s / 10iters, (0.007129)\n",
            "Loss_D = 0.79072583 (ave = 1.33225489)\n",
            "Loss_G = 5.88259315 (ave = 5.27163682)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [11/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.162s / 8iters, (0.020)\tData load 0.029s / 8iters, (0.003610)\n",
            "Loss_D = 0.02573021 (ave = 1.41437617)\n",
            "Loss_G = 3.96787500 (ave = 5.16511055)\n",
            "\n",
            "Real Accuracy : 68.0\n",
            "Fake Accuracy : 4.470588235294118\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [12/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.091s / 10iters, (0.509)\tData load 4.507s / 10iters, (0.450716)\n",
            "Loss_D = 0.68827868 (ave = 0.96881473)\n",
            "Loss_G = 4.76454973 (ave = 5.00771704)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [12/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.406s / 10iters, (0.041)\tData load 0.045s / 10iters, (0.004532)\n",
            "Loss_D = 0.66430968 (ave = 1.00986322)\n",
            "Loss_G = 4.79456520 (ave = 5.05270493)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [12/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.563s / 10iters, (0.056)\tData load 0.115s / 10iters, (0.011481)\n",
            "Loss_D = 0.61027801 (ave = 1.07390181)\n",
            "Loss_G = 4.03714943 (ave = 5.15924007)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [12/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.417s / 10iters, (0.042)\tData load 0.043s / 10iters, (0.004329)\n",
            "Loss_D = 0.95807093 (ave = 1.12038753)\n",
            "Loss_G = 4.40684462 (ave = 4.98036703)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [12/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.166s / 8iters, (0.021)\tData load 0.029s / 8iters, (0.003587)\n",
            "Loss_D = 0.96716577 (ave = 1.08264521)\n",
            "Loss_G = 1.82404053 (ave = 4.84026763)\n",
            "\n",
            "Real Accuracy : 72.23529411764706\n",
            "Fake Accuracy : 3.5294117647058822\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [13/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.208s / 10iters, (0.521)\tData load 4.565s / 10iters, (0.456521)\n",
            "Loss_D = 1.23108220 (ave = 1.24001530)\n",
            "Loss_G = 3.76283979 (ave = 5.18680246)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [13/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.477s / 10iters, (0.048)\tData load 0.064s / 10iters, (0.006421)\n",
            "Loss_D = 1.93681169 (ave = 1.27485959)\n",
            "Loss_G = 9.50604534 (ave = 5.19550270)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [13/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.541s / 10iters, (0.054)\tData load 0.063s / 10iters, (0.006327)\n",
            "Loss_D = 0.84154660 (ave = 1.28090796)\n",
            "Loss_G = 7.05966425 (ave = 5.06283935)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [13/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.396s / 10iters, (0.040)\tData load 0.038s / 10iters, (0.003804)\n",
            "Loss_D = 0.79706329 (ave = 1.33224759)\n",
            "Loss_G = 4.09070587 (ave = 4.90532646)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [13/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.174s / 8iters, (0.022)\tData load 0.033s / 8iters, (0.004172)\n",
            "Loss_D = 1.73154640 (ave = 1.30352853)\n",
            "Loss_G = 13.53537560 (ave = 5.10040248)\n",
            "\n",
            "Real Accuracy : 71.05882352941177\n",
            "Fake Accuracy : 4.705882352941177\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [14/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.168s / 10iters, (0.517)\tData load 4.556s / 10iters, (0.455561)\n",
            "Loss_D = 0.10478114 (ave = 1.12409451)\n",
            "Loss_G = 4.75446129 (ave = 4.39468236)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [14/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.551s / 10iters, (0.055)\tData load 0.076s / 10iters, (0.007551)\n",
            "Loss_D = 0.88900375 (ave = 1.17323977)\n",
            "Loss_G = 4.71822929 (ave = 4.49903076)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [14/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.529s / 10iters, (0.053)\tData load 0.071s / 10iters, (0.007140)\n",
            "Loss_D = 1.60831833 (ave = 1.11600868)\n",
            "Loss_G = 4.23257542 (ave = 4.47507772)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [14/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.324s / 10iters, (0.032)\tData load 0.039s / 10iters, (0.003916)\n",
            "Loss_D = 0.71389657 (ave = 1.06356524)\n",
            "Loss_G = 2.98435116 (ave = 4.55290235)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [14/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.155s / 8iters, (0.019)\tData load 0.032s / 8iters, (0.003955)\n",
            "Loss_D = 1.68488050 (ave = 1.09004671)\n",
            "Loss_G = 3.34626126 (ave = 4.50310734)\n",
            "\n",
            "Real Accuracy : 73.41176470588235\n",
            "Fake Accuracy : 2.823529411764706\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [15/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.310s / 10iters, (0.531)\tData load 4.543s / 10iters, (0.454299)\n",
            "Loss_D = 1.33554709 (ave = 1.23611909)\n",
            "Loss_G = 5.97388220 (ave = 6.43894911)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [15/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.432s / 10iters, (0.043)\tData load 0.057s / 10iters, (0.005697)\n",
            "Loss_D = 0.99143791 (ave = 1.12259169)\n",
            "Loss_G = 5.59343386 (ave = 5.60802535)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [15/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.487s / 10iters, (0.049)\tData load 0.044s / 10iters, (0.004371)\n",
            "Loss_D = 1.02836490 (ave = 1.10344496)\n",
            "Loss_G = 4.79975748 (ave = 5.15912456)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [15/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.420s / 10iters, (0.042)\tData load 0.051s / 10iters, (0.005142)\n",
            "Loss_D = 1.27545834 (ave = 1.03930985)\n",
            "Loss_G = 2.93237805 (ave = 4.96923002)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [15/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.180s / 8iters, (0.022)\tData load 0.026s / 8iters, (0.003292)\n",
            "Loss_D = 0.55982310 (ave = 0.99957888)\n",
            "Loss_G = 5.52974796 (ave = 4.89351083)\n",
            "\n",
            "Real Accuracy : 75.29411764705883\n",
            "Fake Accuracy : 3.0588235294117645\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [16/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.182s / 10iters, (0.518)\tData load 4.580s / 10iters, (0.458012)\n",
            "Loss_D = 0.98719478 (ave = 0.94599887)\n",
            "Loss_G = 6.12596703 (ave = 4.62903669)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [16/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.645s / 10iters, (0.065)\tData load 0.087s / 10iters, (0.008695)\n",
            "Loss_D = 0.80760545 (ave = 0.99507512)\n",
            "Loss_G = 5.61019897 (ave = 4.69833130)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [16/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.467s / 10iters, (0.047)\tData load 0.058s / 10iters, (0.005779)\n",
            "Loss_D = 0.98745328 (ave = 0.95157140)\n",
            "Loss_G = 6.55118370 (ave = 4.69084338)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [16/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.368s / 10iters, (0.037)\tData load 0.054s / 10iters, (0.005364)\n",
            "Loss_D = 1.58602715 (ave = 0.94828772)\n",
            "Loss_G = 5.74473000 (ave = 4.80287667)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [16/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.167s / 8iters, (0.021)\tData load 0.024s / 8iters, (0.003000)\n",
            "Loss_D = 0.62582010 (ave = 0.95190838)\n",
            "Loss_G = 4.03639412 (ave = 4.81005623)\n",
            "\n",
            "Real Accuracy : 77.17647058823529\n",
            "Fake Accuracy : 2.1176470588235294\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [17/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.156s / 10iters, (0.516)\tData load 4.440s / 10iters, (0.443957)\n",
            "Loss_D = 1.12159038 (ave = 1.01714171)\n",
            "Loss_G = 4.57696724 (ave = 4.51817632)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [17/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.626s / 10iters, (0.063)\tData load 0.077s / 10iters, (0.007668)\n",
            "Loss_D = 2.06941509 (ave = 1.02246268)\n",
            "Loss_G = 7.21808386 (ave = 4.52034543)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [17/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.505s / 10iters, (0.051)\tData load 0.032s / 10iters, (0.003236)\n",
            "Loss_D = 0.79379487 (ave = 0.95972726)\n",
            "Loss_G = 4.80520725 (ave = 4.49117519)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [17/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.459s / 10iters, (0.046)\tData load 0.067s / 10iters, (0.006744)\n",
            "Loss_D = 0.83058518 (ave = 1.02981251)\n",
            "Loss_G = 3.89208269 (ave = 4.49428354)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [17/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.171s / 8iters, (0.021)\tData load 0.037s / 8iters, (0.004661)\n",
            "Loss_D = 0.45177254 (ave = 1.01694272)\n",
            "Loss_G = 10.47732544 (ave = 4.61677728)\n",
            "\n",
            "Real Accuracy : 74.11764705882354\n",
            "Fake Accuracy : 4.705882352941177\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [18/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.137s / 10iters, (0.514)\tData load 4.527s / 10iters, (0.452709)\n",
            "Loss_D = 0.49455178 (ave = 0.68942459)\n",
            "Loss_G = 3.77569580 (ave = 3.95846145)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [18/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.528s / 10iters, (0.053)\tData load 0.062s / 10iters, (0.006245)\n",
            "Loss_D = 0.87301689 (ave = 0.76650820)\n",
            "Loss_G = 8.37113762 (ave = 4.27928266)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [18/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.464s / 10iters, (0.046)\tData load 0.053s / 10iters, (0.005322)\n",
            "Loss_D = 0.93501890 (ave = 0.82935906)\n",
            "Loss_G = 2.60624194 (ave = 4.34907683)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [18/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.448s / 10iters, (0.045)\tData load 0.082s / 10iters, (0.008238)\n",
            "Loss_D = 0.71649146 (ave = 0.82980540)\n",
            "Loss_G = 3.36411285 (ave = 4.35526251)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [18/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.153s / 8iters, (0.019)\tData load 0.032s / 8iters, (0.004007)\n",
            "Loss_D = 3.65919352 (ave = 0.91501089)\n",
            "Loss_G = 4.50567341 (ave = 4.54669750)\n",
            "\n",
            "Real Accuracy : 80.47058823529412\n",
            "Fake Accuracy : 3.764705882352941\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [19/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 5.117s / 10iters, (0.512)\tData load 4.628s / 10iters, (0.462796)\n",
            "Loss_D = 0.49422032 (ave = 1.29342556)\n",
            "Loss_G = 5.94776678 (ave = 5.19086232)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [19/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.525s / 10iters, (0.052)\tData load 0.060s / 10iters, (0.005959)\n",
            "Loss_D = 0.78099966 (ave = 1.05071831)\n",
            "Loss_G = 7.51442814 (ave = 5.22047828)\n",
            "\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [19/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.558s / 10iters, (0.056)\tData load 0.064s / 10iters, (0.006443)\n",
            "Loss_D = 0.81382251 (ave = 1.11227058)\n",
            "Loss_G = 4.40167093 (ave = 5.19634244)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "epoch: [19/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.405s / 10iters, (0.040)\tData load 0.058s / 10iters, (0.005769)\n",
            "Loss_D = 0.98707497 (ave = 1.08199702)\n",
            "Loss_G = 6.24704409 (ave = 5.25966597)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [19/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.175s / 8iters, (0.022)\tData load 0.037s / 8iters, (0.004584)\n",
            "Loss_D = 1.41476214 (ave = 1.04249488)\n",
            "Loss_G = 11.65359497 (ave = 5.35220280)\n",
            "\n",
            "Real Accuracy : 75.76470588235294\n",
            "Fake Accuracy : 2.1176470588235294\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [20/20] iteration: [10/48]\tLearning rate: 0.0002\n",
            "Time 4.922s / 10iters, (0.492)\tData load 4.433s / 10iters, (0.443268)\n",
            "Loss_D = 0.97004652 (ave = 0.97887542)\n",
            "Loss_G = 4.96565390 (ave = 5.49711943)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [20/20] iteration: [20/48]\tLearning rate: 0.0002\n",
            "Time 0.572s / 10iters, (0.057)\tData load 0.095s / 10iters, (0.009528)\n",
            "Loss_D = 1.01858330 (ave = 0.86673387)\n",
            "Loss_G = 6.86209583 (ave = 5.15529590)\n",
            "\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "epoch: [20/20] iteration: [30/48]\tLearning rate: 0.0002\n",
            "Time 0.664s / 10iters, (0.066)\tData load 0.052s / 10iters, (0.005161)\n",
            "Loss_D = 0.73175275 (ave = 0.84337024)\n",
            "Loss_G = 6.63307190 (ave = 5.14427991)\n",
            "\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "epoch: [20/20] iteration: [40/48]\tLearning rate: 0.0002\n",
            "Time 0.454s / 10iters, (0.045)\tData load 0.055s / 10iters, (0.005482)\n",
            "Loss_D = 2.13870788 (ave = 0.91272635)\n",
            "Loss_G = 9.98496532 (ave = 5.27299919)\n",
            "\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "epoch: [20/20] iteration: [48/48]\tLearning rate: 0.0002\n",
            "Time 0.171s / 8iters, (0.021)\tData load 0.031s / 8iters, (0.003879)\n",
            "Loss_D = 1.21766520 (ave = 0.91706950)\n",
            "Loss_G = 2.00408602 (ave = 5.12896458)\n",
            "\n",
            "Real Accuracy : 75.76470588235294\n",
            "Fake Accuracy : 3.2941176470588234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NNhwNi5BSr0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}