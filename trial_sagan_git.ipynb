{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "trial_sagan_git.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abbyambita/Diagnosing-COVID-from-CT-Scan-Images/blob/main/trial_sagan_git.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrbY_X-nvYFM",
        "outputId": "d0804d3f-b043-47a2-bee3-f4e7cd2d0f14"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncGTysumvhSl",
        "outputId": "7228f2bd-1d8a-4480-82c6-55e8799d58c8"
      },
      "source": [
        "import os \r\n",
        "\r\n",
        "os.chdir(\"/content/gdrive/My Drive\")\r\n",
        "\r\n",
        "!ls  '/content/gdrive/My Drive/CS 284 Mini-Project/Code/'\r\n",
        "\r\n",
        "%cd \"/content/gdrive/My Drive/CS 284 Mini-Project/Code/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ACGAN.ipynb\t\t       model_result\n",
            " acgan_output_images\t       models\n",
            " BAGAN.ipynb\t\t       new_sars_cov\n",
            " bagan_output_images\t       output_result\n",
            " build_datasets.ipynb\t       plots\n",
            " build_datasets_v2.ipynb       revised-git\n",
            " build_datasets_v3.ipynb       revised-kaggle-validation\n",
            " COVID-CT-master\t       runs\n",
            "'data (can be discarded)'     'synthetic images'\n",
            " DCGAN.ipynb\t\t       training-from-scratch-git.ipynb\n",
            " ECN_git.ipynb\t\t      'transfer learning models (backup trash)'\n",
            " ECN_git_with_acgan.ipynb      transfer_learning_models_git.ipynb\n",
            " ECN_git_with_bagan.ipynb      transfer_learning_models_kaggle.ipynb\n",
            " ECN_kaggle.ipynb\t       transfer_learning_with_acgan_git.ipynb\n",
            " ECN_kaggle_with_acgan.ipynb   transfer_learning_with_acgan_kaggle.ipynb\n",
            " ECN_kaggle_with_bagan.ipynb   transfer_learning_with_bagan_git.ipynb\n",
            " EN_git.ipynb\t\t       transfer_learning_with_bagan_kaggle.ipynb\n",
            " EN_git_with_acgan.ipynb       trial_acgan_git.ipynb\n",
            " EN_git_with_bagan.ipynb       trial_acgan_kaggle.ipynb\n",
            " EN_kaggle.ipynb\t       trial_bagan_git.ipynb\n",
            " EN_kaggle_with_acgan.ipynb    trial_bagan_kaggle.ipynb\n",
            " EN_kaggle_with_bagan.ipynb    trial_GAN.ipynb\n",
            " images.zip\t\t       trial.ipynb\n",
            " Inception_github.ipynb        trial_sagan_git.ipynb\n",
            " master.ipynb\t\t       tsne_acgan_git.ipynb\n",
            " model_backup\t\t       tsne_acgan_kaggle.ipynb\n",
            "/content/gdrive/.shortcut-targets-by-id/1eVFVz23F6ROX0s10Oe3tT9HVzr502iW2/CS 284 Mini-Project/Code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu35A4u7wEDx"
      },
      "source": [
        "from __future__ import print_function\r\n",
        "#%matplotlib inline\r\n",
        "import argparse\r\n",
        "import os\r\n",
        "import PIL\r\n",
        "import glob\r\n",
        "import xml.etree.ElementTree as ET\r\n",
        "import random\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.parallel\r\n",
        "import torch.backends.cudnn as cudnn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data\r\n",
        "import torchvision.datasets as dset\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.utils as vutils\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.animation as animation\r\n",
        "import seaborn as sns\r\n",
        "from IPython.display import HTML\r\n",
        "from torchvision.utils import save_image\r\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\r\n",
        "from tqdm import tqdm_notebook as tqdm\r\n",
        "from IPython.display import clear_output\r\n",
        "from scipy.stats import truncnorm\r\n",
        "%matplotlib inline\r\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision import transforms\r\n",
        "\r\n",
        "import torch\r\n",
        "from torch.optim.optimizer import Optimizer, required\r\n",
        "\r\n",
        "from torch.autograd import Variable\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch import nn\r\n",
        "from torch import Tensor\r\n",
        "from torch.nn import Parameter"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF2S8MPFYpIA"
      },
      "source": [
        "def l2normalize(v, eps=1e-12):\r\n",
        "    return v / (v.norm() + eps)\r\n",
        "\r\n",
        "\r\n",
        "class SpectralNorm(nn.Module):\r\n",
        "    def __init__(self, module, name='weight', power_iterations=1):\r\n",
        "        super(SpectralNorm, self).__init__()\r\n",
        "        self.module = module\r\n",
        "        self.name = name\r\n",
        "        self.power_iterations = power_iterations\r\n",
        "        if not self._made_params():\r\n",
        "            self._make_params()\r\n",
        "\r\n",
        "    def _update_u_v(self):\r\n",
        "        u = getattr(self.module, self.name + \"_u\")\r\n",
        "        v = getattr(self.module, self.name + \"_v\")\r\n",
        "        w = getattr(self.module, self.name + \"_bar\")\r\n",
        "\r\n",
        "        height = w.data.shape[0]\r\n",
        "        for _ in range(self.power_iterations):\r\n",
        "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\r\n",
        "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\r\n",
        "\r\n",
        "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\r\n",
        "        sigma = u.dot(w.view(height, -1).mv(v))\r\n",
        "        setattr(self.module, self.name, w / sigma.expand_as(w))\r\n",
        "\r\n",
        "    def _made_params(self):\r\n",
        "        try:\r\n",
        "            u = getattr(self.module, self.name + \"_u\")\r\n",
        "            v = getattr(self.module, self.name + \"_v\")\r\n",
        "            w = getattr(self.module, self.name + \"_bar\")\r\n",
        "            return True\r\n",
        "        except AttributeError:\r\n",
        "            return False\r\n",
        "\r\n",
        "\r\n",
        "    def _make_params(self):\r\n",
        "        w = getattr(self.module, self.name)\r\n",
        "\r\n",
        "        height = w.data.shape[0]\r\n",
        "        width = w.view(height, -1).data.shape[1]\r\n",
        "\r\n",
        "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\r\n",
        "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\r\n",
        "        u.data = l2normalize(u.data)\r\n",
        "        v.data = l2normalize(v.data)\r\n",
        "        w_bar = Parameter(w.data)\r\n",
        "\r\n",
        "        del self.module._parameters[self.name]\r\n",
        "\r\n",
        "        self.module.register_parameter(self.name + \"_u\", u)\r\n",
        "        self.module.register_parameter(self.name + \"_v\", v)\r\n",
        "        self.module.register_parameter(self.name + \"_bar\", w_bar)\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, *args):\r\n",
        "        self._update_u_v()\r\n",
        "        return self.module.forward(*args)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL8nJimvY2y2"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.autograd import Variable\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class Self_Attn(nn.Module):\r\n",
        "    \"\"\" Self attention Layer\"\"\"\r\n",
        "    def __init__(self, in_dim):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        # Construct the module\r\n",
        "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//2 , kernel_size= 1)\r\n",
        "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//2 , kernel_size= 1)\r\n",
        "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\r\n",
        "        \r\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\r\n",
        "        self.softmax  = nn.Softmax(dim=-1)\r\n",
        "        \r\n",
        "    def forward(self,x):\r\n",
        "        \"\"\"\r\n",
        "            inputs :\r\n",
        "                x : input feature maps( B * C * W * H)\r\n",
        "            returns :\r\n",
        "                out : self attention value + input feature \r\n",
        "                attention: B * N * N (N is Width*Height)\r\n",
        "        \"\"\"\r\n",
        "        m_batchsize,C,width ,height = x.size()\r\n",
        "        \r\n",
        "        proj_query  = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0,2,1) # B * N * C\r\n",
        "        proj_key =  self.key_conv(x).view(m_batchsize, -1, width*height) # B * C * N\r\n",
        "        energy =  torch.bmm(proj_query, proj_key) # batch matrix-matrix product\r\n",
        "        \r\n",
        "        attention = self.softmax(energy) # B * N * N\r\n",
        "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height) # B * C * N\r\n",
        "        out = torch.bmm(proj_value, attention.permute(0,2,1)) # batch matrix-matrix product\r\n",
        "        out = out.view(m_batchsize,C,width,height) # B * C * W * H\r\n",
        "        \r\n",
        "        out = self.gamma*out + x\r\n",
        "        return out, attention\r\n",
        "\r\n",
        "class Generator(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Generator\r\n",
        "    input: \r\n",
        "        z: latent matrix with shape of (batch_size, 100)\r\n",
        "    output: \r\n",
        "        out: generated image with shape (batch_size, 1, 64, 64)\r\n",
        "        p1: attention matrix generated by attn layer\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, batch_size=64, attn=True, image_size=64, z_dim=100, conv_dim=64):\r\n",
        "        super().__init__()\r\n",
        "        self.attn = attn\r\n",
        "        \r\n",
        "        # Layer 1 turn 100 dims -> 512 dims, size 1 -> 4\r\n",
        "        layer1 = []\r\n",
        "        layer1.append(SpectralNorm(nn.ConvTranspose2d(in_channels = z_dim, out_channels = conv_dim*8, kernel_size = 4)))\r\n",
        "        layer1.append(nn.BatchNorm2d(conv_dim*8))\r\n",
        "        layer1.append(nn.ReLU())\r\n",
        "        self.l1 = nn.Sequential(*layer1)\r\n",
        "        \r\n",
        "        # Layer 2 turn 512 dims -> 256 dims, size 4 -> 8\r\n",
        "        layer2 = []\r\n",
        "        layer2.append(SpectralNorm(nn.ConvTranspose2d(in_channels = conv_dim*8, out_channels = conv_dim*4, \r\n",
        "                                                      kernel_size = 4, stride = 2, padding = 1)))\r\n",
        "        layer2.append(nn.BatchNorm2d(conv_dim*4))\r\n",
        "        layer2.append(nn.ReLU())\r\n",
        "        self.l2 = nn.Sequential(*layer2)\r\n",
        "        \r\n",
        "        # Layer 3 turn 256 dims -> 128 dims, size 8 -> 16\r\n",
        "        layer3 = []\r\n",
        "        layer3.append(SpectralNorm(nn.ConvTranspose2d(in_channels = conv_dim*4, out_channels = conv_dim*2, \r\n",
        "                                                      kernel_size = 4, stride = 2, padding = 1)))\r\n",
        "        layer3.append(nn.BatchNorm2d(conv_dim*2))\r\n",
        "        layer3.append(nn.ReLU())\r\n",
        "        self.l3 = nn.Sequential(*layer3)\r\n",
        "\r\n",
        "        # Attn1 layer turn 128 dims -> 128 dims\r\n",
        "        self.attn1 = Self_Attn(conv_dim*2)\r\n",
        "        \r\n",
        "        # Layer 4 turn 128 dims -> 64 dims, size 16 -> 32\r\n",
        "        layer4 = []\r\n",
        "        layer4.append(SpectralNorm(nn.ConvTranspose2d(in_channels = conv_dim*2, out_channels = conv_dim, \r\n",
        "                                                      kernel_size = 4, stride = 2, padding = 1)))\r\n",
        "        layer4.append(nn.BatchNorm2d(conv_dim))\r\n",
        "        layer4.append(nn.ReLU())\r\n",
        "        self.l4 = nn.Sequential(*layer4)\r\n",
        "        \r\n",
        "        # Attn2 layer turn 64 dims -> 64 dims\r\n",
        "        self.attn2 = Self_Attn(conv_dim)\r\n",
        "        \r\n",
        "        # Layer 5 turn 64 dims -> 3 dims, size 32 -> 64\r\n",
        "        layer5 = []\r\n",
        "        layer5.append(nn.ConvTranspose2d(conv_dim, 3, 4, 2, 1))\r\n",
        "        layer5.append(nn.Tanh())\r\n",
        "        self.l5 = nn.Sequential(*layer5)\r\n",
        "        \r\n",
        "\r\n",
        "    def forward(self, z):\r\n",
        "        # z is the input random matrix for generator\r\n",
        "        z = z.view(z.size(0), z.size(1), 1, 1)\r\n",
        "        out=self.l1(z)\r\n",
        "        out=self.l2(out)\r\n",
        "        out=self.l3(out)\r\n",
        "        if self.attn == True:\r\n",
        "            out,_ = self.attn1(out)\r\n",
        "        out=self.l4(out)\r\n",
        "        if self.attn == True:\r\n",
        "            out,_ = self.attn2(out)\r\n",
        "        out=self.l5(out)\r\n",
        "\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class Discriminator(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Discriminator\r\n",
        "    input:\r\n",
        "        x: one batch of data with shape of (batch_size, 1, 64, 64)\r\n",
        "    output: \r\n",
        "        out.squeeze: a batch of scalars indicating the predict results\r\n",
        "        p1: attention matrix generated by attn layer\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, batch_size=64, attn=True, image_size=64, conv_dim=64):\r\n",
        "        super().__init__()\r\n",
        "        self.attn = attn\r\n",
        "        \r\n",
        "        # Layer 1 turn 3 dims -> 64 dims, size 64 -> 32\r\n",
        "        layer1 = []\r\n",
        "        layer1.append(SpectralNorm(nn.Conv2d(3, conv_dim, 4, 2, 1)))\r\n",
        "        layer1.append(nn.LeakyReLU(0.1))\r\n",
        "        curr_dim = conv_dim\r\n",
        "        self.l1 = nn.Sequential(*layer1)\r\n",
        "        \r\n",
        "        # Layer 2 turn 64 dims -> 128 dims, size 32 -> 16\r\n",
        "        layer2 = []\r\n",
        "        layer2.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\r\n",
        "        layer2.append(nn.LeakyReLU(0.1))\r\n",
        "        curr_dim = curr_dim * 2\r\n",
        "        self.l2 = nn.Sequential(*layer2)\r\n",
        "        \r\n",
        "        # Layer 3 turn 128 dims -> 256 dims, size 16 -> 8\r\n",
        "        layer3 = []\r\n",
        "        layer3.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\r\n",
        "        layer3.append(nn.LeakyReLU(0.1))\r\n",
        "        curr_dim = curr_dim * 2\r\n",
        "        self.l3 = nn.Sequential(*layer3)\r\n",
        "        \r\n",
        "        # Attn1 layer remains the same dim and size\r\n",
        "        self.attn1 = Self_Attn(curr_dim)\r\n",
        "        \r\n",
        "        # Layer 4 turn 256 dims -> 512 dims, size 8 -> 4\r\n",
        "        layer4 = []\r\n",
        "        layer4.append(SpectralNorm(nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1)))\r\n",
        "        layer4.append(nn.LeakyReLU(0.1))\r\n",
        "        curr_dim = curr_dim * 2\r\n",
        "        self.l4 = nn.Sequential(*layer4)\r\n",
        "        \r\n",
        "        # Attn2 layer remains the same dim and size\r\n",
        "        self.attn2 = Self_Attn(curr_dim)\r\n",
        "        \r\n",
        "        # Layer 5 turn 512 dims -> 1 dims, size 4 -> 1\r\n",
        "        layer5 = []\r\n",
        "        layer5.append(nn.Conv2d(curr_dim, 1, 4, 1, 0))\r\n",
        "        self.l5 = nn.Sequential(*layer5)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        out = self.l1(x)\r\n",
        "        out = self.l2(out)\r\n",
        "        out = self.l3(out)\r\n",
        "        if self.attn == True:\r\n",
        "            out,_ = self.attn1(out)\r\n",
        "        out = self.l4(out)\r\n",
        "        if self.attn == True:\r\n",
        "            out,_ = self.attn2(out)\r\n",
        "        out = self.l5(out)\r\n",
        "\r\n",
        "        return out.squeeze()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf5E9tovZBxI"
      },
      "source": [
        "from IPython.display import clear_output\r\n",
        "import datetime\r\n",
        "import time\r\n",
        "import os"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D58t3IfNW_pC"
      },
      "source": [
        "def load_images(batch_size):\r\n",
        "  dataroot = \"revised-git/train\"\r\n",
        "\r\n",
        "  dataset = dset.ImageFolder(root=dataroot,\r\n",
        "                            transform=transforms.Compose([\r\n",
        "                                transforms.Resize(image_size),\r\n",
        "                                transforms.RandomHorizontalFlip(),\r\n",
        "                                transforms.CenterCrop(image_size),\r\n",
        "                                transforms.ToTensor(),\r\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\r\n",
        "                            ]))\r\n",
        "  \r\n",
        "  print(len(dataset))\r\n",
        "\r\n",
        "  dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True, num_workers=workers)\r\n",
        "\r\n",
        "  return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT3TlJXhZTEn",
        "outputId": "40cb7f50-9a35-4eaa-de4d-52314acac3d4"
      },
      "source": [
        "batch_size = 64\r\n",
        "\r\n",
        "# Utility functions\r\n",
        "def cuda(data):\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        return data.cuda()\r\n",
        "    else:\r\n",
        "        return data\r\n",
        "\r\n",
        "def denorm(x):\r\n",
        "    out = (x + 1) / 2\r\n",
        "    return out.clamp_(0, 1)\r\n",
        "\r\n",
        "transform = transforms.Compose([transforms.Resize(64),\r\n",
        "                                transforms.CenterCrop(64),\r\n",
        "                                transforms.ToTensor(),\r\n",
        "                                transforms.Normalize([0.5]*3,[0.5]*3)])\r\n",
        "\r\n",
        "#train_data = ImageFolder('./data/celebA', transform=transform)\r\n",
        "\r\n",
        "dataroot = \"revised-git/train\"\r\n",
        "\r\n",
        "train_data = dset.ImageFolder(root=dataroot,\r\n",
        "                            transform=transform)\r\n",
        "\r\n",
        "print(len(train_data))\r\n",
        "dataloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=4)\r\n",
        "\r\n",
        "# Fix a random latent input for samples\r\n",
        "fixed_z = cuda(torch.randn(64, 100))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubw33vxnZ4Dd"
      },
      "source": [
        "def train(steps = 5000, batch_size = 64, z_dim = 100, attn = True):\r\n",
        "    # Initialize model\r\n",
        "    G = cuda(Generator(batch_size, attn))\r\n",
        "    D = cuda(Discriminator(batch_size, attn))\r\n",
        "    \r\n",
        "    # Make directory for samples and models\r\n",
        "    cwd = os.getcwd()\r\n",
        "    post='_attn' if attn else ''\r\n",
        "    if not os.path.exists(cwd+'/samples_celeba'+post):\r\n",
        "        os.makedirs(cwd+'/samples_celeba'+post)\r\n",
        "\r\n",
        "    # Initialize optimizer with filter, lr and coefficients\r\n",
        "    g_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, G.parameters()), 0.0001, [0.0,0.9])\r\n",
        "    d_optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, D.parameters()), 0.0004, [0.0,0.9])\r\n",
        "    \r\n",
        "    # Load data\r\n",
        "    Iter = iter(dataloader)\r\n",
        "    \r\n",
        "    # Start timer\r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    for step in range(steps):\r\n",
        "        # ================== Train D ================== #\r\n",
        "        D.train(); G.train()\r\n",
        "        try:\r\n",
        "            real_images,_ = next(Iter)\r\n",
        "        except:\r\n",
        "            Iter = iter(dataloader)\r\n",
        "            real_images,_ = next(Iter)\r\n",
        "        \r\n",
        "        # Compute loss with real images\r\n",
        "        d_out_real = D(cuda(real_images))\r\n",
        "        d_loss_real = torch.nn.ReLU()(1.0 - d_out_real).mean()\r\n",
        "        \r\n",
        "        # Compute loss with fake images\r\n",
        "        z = cuda(torch.randn(batch_size, z_dim))\r\n",
        "        fake_images = G(z)\r\n",
        "        d_out_fake = D(fake_images)\r\n",
        "        d_loss_fake = torch.nn.ReLU()(1.0 + d_out_fake).mean()\r\n",
        "        \r\n",
        "        # Backward + Optimize\r\n",
        "        d_loss = d_loss_real + d_loss_fake\r\n",
        "        d_optimizer.zero_grad(); g_optimizer.zero_grad()\r\n",
        "        d_loss.backward()\r\n",
        "        d_optimizer.step()\r\n",
        "        \r\n",
        "        # ================== Train G ================== #\r\n",
        "        # Create random noise\r\n",
        "        z = cuda(torch.randn(batch_size, z_dim))\r\n",
        "        fake_images = G(z)\r\n",
        "        g_out_fake = D(fake_images)\r\n",
        "        g_loss_fake = - g_out_fake.mean()\r\n",
        "        d_optimizer.zero_grad(); g_optimizer.zero_grad()\r\n",
        "        g_loss_fake.backward()\r\n",
        "        g_optimizer.step()\r\n",
        "        \r\n",
        "        # Print out log info\r\n",
        "        if (step + 1) % 10 == 0:\r\n",
        "            elapsed = time.time() - start_time\r\n",
        "            expect = elapsed/(step + 1)*(steps-step-1)\r\n",
        "            elapsed = str(datetime.timedelta(seconds=elapsed))\r\n",
        "            expect = str(datetime.timedelta(seconds=expect))\r\n",
        "            clear_output(wait=True)\r\n",
        "            print(\"Elapsed [{}], Expect [{}], step [{}/{}], D_real_loss: {:.4f}, \"\r\n",
        "                  \" ave_generator_gamma1: {:.4f}, ave_generator_gamma2: {:.4f}\".\r\n",
        "                  format(elapsed,expect,step + 1,steps,d_loss_real.item(),\r\n",
        "                         G.attn1.gamma.mean().item(),\r\n",
        "                         G.attn2.gamma.mean().item()))\r\n",
        "        \r\n",
        "        # Sample images\r\n",
        "        if (step + 1) % (100) == 0:\r\n",
        "            fake_images= G(fixed_z)\r\n",
        "            save_image(denorm(fake_images), os.path.join('./samples_celeba'+post, '{}_fake.png'.format(step + 1)))\r\n",
        "        \r\n",
        "        # Save models\r\n",
        "        #if (step+1) % (100) == 0:\r\n",
        "            #torch.save(G.state_dict(),os.path.join('./models', '{}_G.pth'.format(step + 1)))\r\n",
        "            #torch.save(D.state_dict(),os.path.join('./models', '{}_D.pth'.format(step + 1)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xXg9kHIaSrG",
        "outputId": "145254b2-f534-4995-c3a8-e2810a87c82c"
      },
      "source": [
        "train(steps = 5000, attn = True)\r\n",
        "print('Done training part 1')\r\n",
        "train(steps = 5000, attn = False)\r\n",
        "print('Done training part 2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed [0:18:07.926673], Expect [0:12:11.348700], step [2990/5000], D_real_loss: 0.0000,  ave_generator_gamma1: -0.0680, ave_generator_gamma2: -0.0588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK7E49C8bGYB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}